{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "229d1070-9828-4b02-8e37-bb8d01af7b75",
      "metadata": {
        "id": "229d1070-9828-4b02-8e37-bb8d01af7b75"
      },
      "source": [
        "# 2024年 世界モデル コンペティション 参考notebook  \n",
        "\n",
        "第8回演習で利用したDreamerに修正を加え，Dreamer v2を用いたベースラインコードになっています．  \n",
        "こちらを動かしていただけば，提出時にエラーが発生しない結果を得ることができます（参考用としてcolabの無料枠で1時間ほどで終わるようにパラメータを変えているため，性能は出ないです）．  \n",
        "\n",
        "**目次**\n",
        "1. [準備](#scrollTo=b986f379-97f5-4449-b4c6-7cc385d1f474)\n",
        "2. [環境の設定](#scrollTo=c7819663-fffc-44e5-842f-779564dd8227)\n",
        "3. [補助機能の実装](#scrollTo=6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae)\n",
        "4. [モデルの実装](#scrollTo=0662612e-701b-41a2-8679-25ad03fef367)\n",
        "5. [学習](#scrollTo=b06c188f-8a87-42e7-9f61-7f385eccc565)\n",
        "6. [モデルの保存](#scrollTo=aa693a51-a4cb-4ad4-be2b-322cbd68443d)\n",
        "7. [学習済みパラメータで評価](#scrollTo=c4b31352-bafa-46ed-8bcc-632a24dfced6)\n",
        "\n",
        "以下良い性能を出すためにできる工夫の例です．  \n",
        "- ハイパーパラメータを調整する．  \n",
        "  - バッチサイズを大きくする．\n",
        "  - 更新回数を増やす（update_freqを小さくする）．\n",
        "  - モデルの次元数を大きくする．  など\n",
        "- Dreamer v2の各モデルのアーキテクチャを変更する．\n",
        "- Dreamer v2以外の学習手法を用いる．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b986f379-97f5-4449-b4c6-7cc385d1f474",
      "metadata": {
        "id": "b986f379-97f5-4449-b4c6-7cc385d1f474"
      },
      "source": [
        "## 1. 準備  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3d2823-ec95-4607-a33f-37bfe410f6b4",
      "metadata": {
        "id": "1c3d2823-ec95-4607-a33f-37bfe410f6b4"
      },
      "source": [
        "必要なライブラリのインストール．各自必要なライブラリがある場合は追加でインストールしてください．  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.26.2 gym[atari]==0.26.2 gym[accept-rom-license]==0.26.2 autorom ale-py"
      ],
      "metadata": {
        "id": "5b9qtVaIMAB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19fc725-0b7b-4b45-f923-a4d7afc0513a"
      },
      "id": "5b9qtVaIMAB-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/721.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/721.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/721.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom) (4.67.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py) (6.5.2)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]==0.26.2)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2025.1.31)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827695 sha256=052050b757390b47c2e158d07632afba0f89756ac541c86445d1ecb8683ef53e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=873318bb59d9b74f8528ab021516a6bfee6f48cffb5754dc704042db851f0006\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.10.1\n",
            "    Uninstalling ale-py-0.10.1:\n",
            "      Successfully uninstalled ale-py-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 gym-0.26.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf",
      "metadata": {
        "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf"
      },
      "source": [
        "### ライブラリインポート  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
      "metadata": {
        "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
        "jupyter": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import ResizeObservation\n",
        "import torch\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Normal, Categorical, OneHotCategorical, OneHotCategoricalStraightThrough\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from einops import rearrange, repeat, reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPUの確認"
      ],
      "metadata": {
        "id": "tk5TF5MW1Jwg"
      },
      "id": "tk5TF5MW1Jwg"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "WQXLmjGY1L4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe38eba-4c86-422b-b6df-c19d7704a7cb"
      },
      "id": "WQXLmjGY1L4u",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wjkagwyF1Ubo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcf6643-0651-4a7c-e0b6-d3486f0a2d54"
      },
      "id": "wjkagwyF1Ubo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Feb 15 02:59:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7819663-fffc-44e5-842f-779564dd8227",
      "metadata": {
        "id": "c7819663-fffc-44e5-842f-779564dd8227"
      },
      "source": [
        "## 2. 環境の設定  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e",
      "metadata": {
        "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e"
      },
      "source": [
        "### Repeat Action  \n",
        "- こちらで実装している環境を用いてOmnicampus上では評価を行います．  \n",
        "- モデルによって変更する可能性があると想定している部分は以下のとおりです．\n",
        "    - 画像のレンダリングサイズ(ResizeObervationクラスのshape)．\n",
        "    - 同じ行動を繰り返す数（RepeatActionクラスのskip）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "976179d0-2365-4440-bd7c-7e07ae218901",
      "metadata": {
        "id": "976179d0-2365-4440-bd7c-7e07ae218901"
      },
      "outputs": [],
      "source": [
        "class RepeatAction(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n",
        "    \"\"\"\n",
        "    def __init__(self, env, skip=4, max_steps=100_000):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.max_steps = max_steps if max_steps else float(\"inf\")  # イテレーションの制限\n",
        "        self.steps = 0  # イテレーション回数のカウント\n",
        "        self.height = env.observation_space.shape[0]\n",
        "        self.width = env.observation_space.shape[1]\n",
        "        self._skip = skip\n",
        "        self.lives_info = None\n",
        "\n",
        "    def reset(self):\n",
        "        obs, info = self.env.reset()\n",
        "        self.lives_info = info['lives']\n",
        "        return obs[0], info\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.steps >= self.max_steps:  # 100kに達したら何も返さないようにする\n",
        "            print(\"Reached max iterations.\")\n",
        "            return None\n",
        "\n",
        "        total_reward = 0.0\n",
        "        self.steps += 1\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, _, info = self.env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            if self.steps >= self.max_steps:  # 100kに達したら終端にする\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # ライフが減ったかチェックする\n",
        "        current_lives_info = info['lives']\n",
        "        if current_lives_info < self.lives_info:\n",
        "            info['life_loss'] = True\n",
        "            self.lives_info = current_lives_info\n",
        "        else:\n",
        "            info['life_loss'] = False\n",
        "\n",
        "        return obs, total_reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae",
      "metadata": {
        "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae"
      },
      "source": [
        "## 3. 補助機能の実装  \n",
        "- モデルを保存する際に利用できるクラス，torchのシード値を固定できる関数です．   \n",
        "- 提出いただくパラメータの保存や読み込みにこちらのクラスを必ず利用する必要はありません  ．"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの保存"
      ],
      "metadata": {
        "id": "W9O16hLxp2kx"
      },
      "id": "W9O16hLxp2kx"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "439e4333-d460-4ef2-9aab-5c6f673f8261",
      "metadata": {
        "id": "439e4333-d460-4ef2-9aab-5c6f673f8261"
      },
      "outputs": [],
      "source": [
        "# モデルパラメータをGoogleDriveに保存・後で読み込みするためのヘルパークラス\n",
        "class TrainedModels:\n",
        "    def __init__(self, *models) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        models : nn.Module\n",
        "            保存するモデル．複数モデルを渡すことができます．\n",
        "\n",
        "        使用例: trained_models = TraindModels(encoder, rssm, value_model, action_model)\n",
        "        \"\"\"\n",
        "        assert np.all([nn.Module in model.__class__.__bases__ for model in models]), \"Arguments for TrainedModels need to be nn models.\"\n",
        "\n",
        "        self.models = models\n",
        "\n",
        "    def save(self, dir: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを保存します．\n",
        "        パラメータのファイル名は01.pt, 02.pt, ... のように連番になっています．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\")\n",
        "            )\n",
        "\n",
        "    def load(self, dir: str, device: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを読み込みます．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        device : str\n",
        "            モデルをどのデバイス(CPU or GPU)に載せるかの設定．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\"),\n",
        "                    map_location=device\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### set seed"
      ],
      "metadata": {
        "id": "M79CuiJdp9NL"
      },
      "id": "M79CuiJdp9NL"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7",
      "metadata": {
        "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=20010105) -> None:\n",
        "    \"\"\"\n",
        "    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        シード値．\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions"
      ],
      "metadata": {
        "id": "Y4pUAwnsQIKd"
      },
      "id": "Y4pUAwnsQIKd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### symlog"
      ],
      "metadata": {
        "id": "V_K6D_x5QXmS"
      },
      "id": "V_K6D_x5QXmS"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def symlog(x):\n",
        "    return torch.sign(x) * torch.log(1 + torch.abs(x))"
      ],
      "metadata": {
        "id": "P3fzmnBeQPI0"
      },
      "id": "P3fzmnBeQPI0",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### symexp"
      ],
      "metadata": {
        "id": "oSlxV1q3QbEf"
      },
      "id": "oSlxV1q3QbEf"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def symexp(x):\n",
        "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)"
      ],
      "metadata": {
        "id": "Qsn9NJCcQekW"
      },
      "id": "Qsn9NJCcQekW",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SymLogLoss"
      ],
      "metadata": {
        "id": "Wd4xRVqjQgqR"
      },
      "id": "Wd4xRVqjQgqR"
    },
    {
      "cell_type": "code",
      "source": [
        "class SymLogLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        target = symlog(target)\n",
        "        return 0.5*F.mse_loss(output, target)"
      ],
      "metadata": {
        "id": "EIKrGHgjQkWI"
      },
      "id": "EIKrGHgjQkWI",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SymLogTwoHotLoss"
      ],
      "metadata": {
        "id": "fkQjCa9YQvBG"
      },
      "id": "fkQjCa9YQvBG"
    },
    {
      "cell_type": "code",
      "source": [
        "class SymLogTwoHotLoss(nn.Module):\n",
        "    def __init__(self, num_classes, lower_bound, upper_bound):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lower_bound = lower_bound\n",
        "        self.upper_bound = upper_bound\n",
        "        self.bin_length = (upper_bound - lower_bound) / (num_classes-1)\n",
        "\n",
        "        # use register buffer so that bins move with .cuda() automatically\n",
        "        self.bins: torch.Tensor\n",
        "        self.register_buffer(\n",
        "            'bins', torch.linspace(-20, 20, num_classes), persistent=False)\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        target = symlog(target) # (B, L)\n",
        "        assert target.min() >= self.lower_bound and target.max() <= self.upper_bound\n",
        "\n",
        "        index = torch.bucketize(target, self.bins)\n",
        "        diff = target - self.bins[index-1]  # -1 to get the lower bound\n",
        "        weight = diff / self.bin_length\n",
        "        weight = torch.clamp(weight, 0, 1)\n",
        "        weight = weight.unsqueeze(-1) # (1, 1)\n",
        "\n",
        "        target_prob = (1-weight)*F.one_hot(index-1, self.num_classes) + weight*F.one_hot(index, self.num_classes)\n",
        "\n",
        "        loss = -target_prob * F.log_softmax(output, dim=-1)\n",
        "        loss = loss.sum(dim=-1)\n",
        "        return loss.mean()\n",
        "\n",
        "    def decode(self, output):\n",
        "        return symexp(F.softmax(output, dim=-1) @ self.bins)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     loss_func = SymLogTwoHotLoss(255, -20, 20)\n",
        "#     output = torch.randn(1, 1, 255).requires_grad_()\n",
        "#     target = torch.ones(1).reshape(1, 1).float() * 0.1\n",
        "#     print(target)\n",
        "#     loss = loss_func(output, target)\n",
        "#     print(loss)\n",
        "\n",
        "    # prob = torch.ones(1, 1, 255)*0.5/255\n",
        "    # prob[0, 0, 128] = 0.5\n",
        "    # logits = torch.log(prob)\n",
        "    # print(loss_func.decode(logits), loss_func.bins[128])"
      ],
      "metadata": {
        "id": "WEhpypVzQyUb"
      },
      "id": "WEhpypVzQyUb",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Blocks"
      ],
      "metadata": {
        "id": "2C8cVcFeC9yI"
      },
      "id": "2C8cVcFeC9yI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get subsequent mask"
      ],
      "metadata": {
        "id": "UaNakMFszCua"
      },
      "id": "UaNakMFszCua"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subsequent_mask(seq):\n",
        "    ''' For masking out the subsequent info. '''\n",
        "    batch_size, batch_length = seq.shape[:2]\n",
        "    subsequent_mask = (1 - torch.triu(\n",
        "        torch.ones((1, batch_length, batch_length), device=seq.device), diagonal=1)).bool() # (1, L, L)\n",
        "    return subsequent_mask"
      ],
      "metadata": {
        "id": "E5QmRteRzGmx"
      },
      "id": "E5QmRteRzGmx",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get subsequent mask with batch length"
      ],
      "metadata": {
        "id": "CVLZHwT-zOyW"
      },
      "id": "CVLZHwT-zOyW"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subsequent_mask_with_batch_length(batch_length, device):\n",
        "    ''' For masking out the subsequent info. '''\n",
        "    subsequent_mask = (1 - torch.triu(torch.ones((1, batch_length, batch_length), device=device), diagonal=1)).bool()\n",
        "    return subsequent_mask"
      ],
      "metadata": {
        "id": "JaG9h_dWzSq2"
      },
      "id": "JaG9h_dWzSq2",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get vector mask"
      ],
      "metadata": {
        "id": "f_fzJrueBGZf"
      },
      "id": "f_fzJrueBGZf"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_mask(batch_length, device):\n",
        "    mask = torch.ones((1, 1, batch_length), device=device).bool()\n",
        "    # mask = torch.ones((1, batch_length, 1), device=device).bool()\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Mya52ngZBOnw"
      },
      "id": "Mya52ngZBOnw",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot Product Attention (QKVの計算)"
      ],
      "metadata": {
        "id": "YuNalqFckCJ3"
      },
      "id": "YuNalqFckCJ3"
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "gNVcxGKUkI13"
      },
      "id": "gNVcxGKUkI13",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention"
      ],
      "metadata": {
        "id": "vm9ZeBgZjTgX"
      },
      "id": "vm9ZeBgZjTgX"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
        "        # Separate different heads: b x lq x n x dv\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        # Transpose for attention dot product: b x n x lq x dv\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
        "\n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # Transpose to move the head dimension back: b x lq x n x dv\n",
        "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "\n",
        "        q = self.layer_norm(q) # (B, L=1, d_model=512)\n",
        "\n",
        "        return q, attn"
      ],
      "metadata": {
        "id": "oVutRLI-jYIE"
      },
      "id": "oVutRLI-jYIE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positionwise Feed Forward"
      ],
      "metadata": {
        "id": "lqKsOpESyRBs"
      },
      "id": "lqKsOpESyRBs"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise\n",
        "        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise\n",
        "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.w_2(F.relu(self.w_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "XH7hkUfGyZ8l"
      },
      "id": "XH7hkUfGyZ8l",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Block KV Cache"
      ],
      "metadata": {
        "id": "pWE9zOsoixkM"
      },
      "id": "pWE9zOsoixkM"
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlockKVCache(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.slf_attn = MultiHeadAttention(num_heads, feat_dim, feat_dim//num_heads, feat_dim//num_heads, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(feat_dim, hidden_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, q, k, v, slf_attn_mask=None):\n",
        "        output, attn = self.slf_attn(q, k, v, mask=slf_attn_mask)\n",
        "        output = self.pos_ffn(output)\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "KdS7V7pei7bq"
      },
      "id": "KdS7V7pei7bq",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding 1D"
      ],
      "metadata": {
        "id": "kGsbTlZyDZzf"
      },
      "id": "kGsbTlZyDZzf"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_length: int,\n",
        "        embed_dim: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_length = max_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.pos_emb = nn.Embedding(self.max_length, embed_dim)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        pos_emb = self.pos_emb(torch.arange(self.max_length, device=feat.device))\n",
        "        pos_emb = repeat(pos_emb, \"L D -> B L D\", B=feat.shape[0])\n",
        "\n",
        "        feat = feat + pos_emb[:, :feat.shape[1], :]\n",
        "        return feat\n",
        "\n",
        "    def forward_with_position(self, feat, position):\n",
        "        assert feat.shape[1] == 1\n",
        "        pos_emb = self.pos_emb(torch.arange(self.max_length, device=feat.device))\n",
        "        pos_emb = repeat(pos_emb, \"L D -> B L D\", B=feat.shape[0])\n",
        "\n",
        "        feat = feat + pos_emb[:, position:position+1, :]\n",
        "        return feat"
      ],
      "metadata": {
        "id": "WQ8ZRhpgEKlo"
      },
      "id": "WQ8ZRhpgEKlo",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Blocks"
      ],
      "metadata": {
        "id": "vf0QKLnaiXX7"
      },
      "id": "vf0QKLnaiXX7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer KV cache\n"
      ],
      "metadata": {
        "id": "KiE468W5CZeW"
      },
      "id": "KiE468W5CZeW"
    },
    {
      "cell_type": "code",
      "source": [
        "class StochasticTransformerKVCache(nn.Module):\n",
        "    def __init__(self, stoch_dim, action_dim, feat_dim, num_layers, num_heads, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.action_dim = action_dim\n",
        "        self.feat_dim = feat_dim\n",
        "\n",
        "        # mix image_embedding and action\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Linear(stoch_dim+action_dim, feat_dim, bias=False),\n",
        "            nn.LayerNorm(feat_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(feat_dim, feat_dim, bias=False),\n",
        "            nn.LayerNorm(feat_dim)\n",
        "        )\n",
        "        self.position_encoding = PositionalEncoding1D(max_length=max_length, embed_dim=feat_dim)\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            AttentionBlockKVCache(feat_dim=feat_dim, hidden_dim=feat_dim*2, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(feat_dim, eps=1e-6)  # TODO: check if this is necessary\n",
        "\n",
        "    def forward(self, samples, action, mask):\n",
        "        '''\n",
        "        Normal forward pass\n",
        "        mask: (1, L, L)\n",
        "        '''\n",
        "        action = F.one_hot(action.long(), self.action_dim).float()\n",
        "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
        "        feats = self.position_encoding(feats)\n",
        "        feats = self.layer_norm(feats)\n",
        "\n",
        "        for layer in self.layer_stack:\n",
        "            feats, attn = layer(feats, feats, feats, mask)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    def reset_kv_cache_list(self, batch_size, dtype):\n",
        "        '''\n",
        "        Reset self.kv_cache_list\n",
        "        '''\n",
        "        self.kv_cache_list = []\n",
        "        for layer in self.layer_stack:\n",
        "            self.kv_cache_list.append(torch.zeros(size=(batch_size, 0, self.feat_dim), dtype=dtype, device=\"cuda\"))\n",
        "\n",
        "    def forward_with_kv_cache(self, samples, action):\n",
        "        '''\n",
        "        Forward pass with kv_cache, cache stored in self.kv_cache_list\n",
        "        samples: last_flattened_sample (B, 1, n_classes*stoch_dim)\n",
        "        '''\n",
        "        assert samples.shape[1] == 1\n",
        "        mask = get_vector_mask(self.kv_cache_list[0].shape[1]+1, samples.device) # self.kv_cache_list[0].shape[1]+1は現在までのbatch_length # (1, 1, batch_length) True\n",
        "\n",
        "        action = F.one_hot(action.long(), self.action_dim).float()\n",
        "        feats = self.stem(torch.cat([samples, action], dim=-1)) # mix image_embedding and action # (B, 1, feat_dim=512)\n",
        "        feats = self.position_encoding.forward_with_position(feats, position=self.kv_cache_list[0].shape[1]) # (B, 1, feat_dim)\n",
        "        feats = self.layer_norm(feats) # (B, 1, feat_dim=512)\n",
        "\n",
        "        for idx, layer in enumerate(self.layer_stack):\n",
        "            self.kv_cache_list[idx] = torch.cat([self.kv_cache_list[idx], feats], dim=1)\n",
        "            feats, attn = layer(feats, self.kv_cache_list[idx], self.kv_cache_list[idx], mask)\n",
        "\n",
        "        return feats"
      ],
      "metadata": {
        "id": "yjXxnYNDCgva"
      },
      "id": "yjXxnYNDCgva",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "fpOoNL3qwDvO"
      },
      "id": "fpOoNL3qwDvO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### percentile"
      ],
      "metadata": {
        "id": "WCT2k8M4wGWK"
      },
      "id": "WCT2k8M4wGWK"
    },
    {
      "cell_type": "code",
      "source": [
        "def percentile(x, percentage):\n",
        "    flat_x = torch.flatten(x)\n",
        "    kth = int(percentage*len(flat_x))\n",
        "    per = torch.kthvalue(flat_x, kth).values\n",
        "    return per"
      ],
      "metadata": {
        "id": "yz9pwbbrwJvb"
      },
      "id": "yz9pwbbrwJvb",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculate λ return"
      ],
      "metadata": {
        "id": "BvB_QQKvwZSV"
      },
      "id": "BvB_QQKvwZSV"
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_lambda_return(rewards, values, termination, gamma, lam, dtype=torch.float32):\n",
        "    # Invert termination to have 0 if the episode ended and 1 otherwise\n",
        "    inv_termination = (termination * -1) + 1\n",
        "\n",
        "    batch_size, batch_length = rewards.shape[:2]\n",
        "    # gae_step = torch.zeros((batch_size, ), dtype=dtype, device=\"cuda\")\n",
        "    gamma_return = torch.zeros((batch_size, batch_length+1), dtype=dtype, device=\"cuda\")\n",
        "    gamma_return[:, -1] = values[:, -1]\n",
        "    for t in reversed(range(batch_length)):  # with last bootstrap\n",
        "        gamma_return[:, t] = \\\n",
        "            rewards[:, t] + \\\n",
        "            gamma * inv_termination[:, t] * (1-lam) * values[:, t] + \\\n",
        "            gamma * inv_termination[:, t] * lam * gamma_return[:, t+1]\n",
        "    return gamma_return[:, :-1]"
      ],
      "metadata": {
        "id": "cvgavQEcwcH5"
      },
      "id": "cvgavQEcwcH5",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor Critic Agent"
      ],
      "metadata": {
        "id": "HBdEre5uw3kd"
      },
      "id": "HBdEre5uw3kd"
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticAgent(nn.Module):\n",
        "    def __init__(self, feat_dim, num_layers, hidden_dim, action_dim, gamma, lambd, entropy_coef) -> None:\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.lambd = lambd\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.use_amp = True\n",
        "        self.tensor_dtype = torch.bfloat16 if self.use_amp else torch.float32\n",
        "\n",
        "        self.symlog_twohot_loss = SymLogTwoHotLoss(255, -20, 20)\n",
        "\n",
        "        actor = [\n",
        "            nn.Linear(feat_dim, hidden_dim, bias=False),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        for i in range(num_layers - 1):\n",
        "            actor.extend([\n",
        "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "        self.actor = nn.Sequential(\n",
        "            *actor,\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        critic = [\n",
        "            nn.Linear(feat_dim, hidden_dim, bias=False),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        for i in range(num_layers - 1):\n",
        "            critic.extend([\n",
        "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            *critic,\n",
        "            nn.Linear(hidden_dim, 255)\n",
        "        )\n",
        "        self.slow_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.lowerbound_ema = EMAScalar(decay=0.99)\n",
        "        self.upperbound_ema = EMAScalar(decay=0.99)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=3e-5, eps=1e-5)\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_slow_critic(self, decay=0.98):\n",
        "        for slow_param, param in zip(self.slow_critic.parameters(), self.critic.parameters()):\n",
        "            slow_param.data.copy_(slow_param.data * decay + param.data * (1 - decay))\n",
        "\n",
        "    def policy(self, x):\n",
        "        logits = self.actor(x)\n",
        "        return logits\n",
        "\n",
        "    def value(self, x):\n",
        "        value = self.critic(x)\n",
        "        value = self.symlog_twohot_loss.decode(value)\n",
        "        return value\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def slow_value(self, x):\n",
        "        value = self.slow_critic(x)\n",
        "        value = self.symlog_twohot_loss.decode(value)\n",
        "        return value\n",
        "\n",
        "    def get_logits_raw_value(self, x):\n",
        "        logits = self.actor(x)\n",
        "        raw_value = self.critic(x)\n",
        "        return logits, raw_value\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, latent, greedy=False):\n",
        "        # latentはpriorとtransformerのhをconcatしたもの\n",
        "        self.eval()\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.use_amp):\n",
        "            logits = self.policy(latent)\n",
        "            dist = distributions.Categorical(logits=logits)\n",
        "            if greedy:\n",
        "                action = dist.probs.argmax(dim=-1)\n",
        "            else:\n",
        "                action = dist.sample()\n",
        "        return action\n",
        "\n",
        "    def sample_as_env_action(self, latent, greedy=False):\n",
        "        action = self.sample(latent, greedy)\n",
        "        return action.detach().cpu().squeeze(-1).item()\n",
        "\n",
        "    def update(self, latent, action, old_logprob, old_value, reward, termination, logger=None):\n",
        "        '''\n",
        "        Update policy and value model\n",
        "        '''\n",
        "        self.train()\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.use_amp):\n",
        "            logits, raw_value = self.get_logits_raw_value(latent) # (B, L+1, action_dim) # (B, L+1, 255)\n",
        "            dist = distributions.Categorical(logits=logits[:, :-1]) # (B, L, action_dim)\n",
        "            log_prob = dist.log_prob(action) # (B, L)\n",
        "            entropy = dist.entropy() # (B, L)\n",
        "\n",
        "            # decode value, calc lambda return\n",
        "            slow_value = self.slow_value(latent) # (B, L+1) # 勾配なし\n",
        "            slow_lambda_return = calc_lambda_return(reward, slow_value, termination, self.gamma, self.lambd) # (B, L)\n",
        "            value = self.symlog_twohot_loss.decode(raw_value) # (B, L+1)\n",
        "            lambda_return = calc_lambda_return(reward, value, termination, self.gamma, self.lambd) # (B, L)\n",
        "\n",
        "            # update value function with slow critic regularization\n",
        "            value_loss = self.symlog_twohot_loss(raw_value[:, :-1], lambda_return.detach()) #\n",
        "            slow_value_regularization_loss = self.symlog_twohot_loss(raw_value[:, :-1], slow_lambda_return.detach())\n",
        "\n",
        "            lower_bound = self.lowerbound_ema(percentile(lambda_return, 0.05))\n",
        "            upper_bound = self.upperbound_ema(percentile(lambda_return, 0.95))\n",
        "            S = upper_bound-lower_bound\n",
        "            norm_ratio = torch.max(torch.ones(1).cuda(), S)  # max(1, S) in the paper\n",
        "            norm_advantage = (lambda_return-value[:, :-1]) / norm_ratio\n",
        "            policy_loss = -(log_prob * norm_advantage.detach()).mean()\n",
        "\n",
        "            entropy_loss = entropy.mean()\n",
        "\n",
        "            loss = policy_loss + value_loss + slow_value_regularization_loss - self.entropy_coef * entropy_loss\n",
        "\n",
        "        # gradient descent\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.unscale_(self.optimizer)  # for clip grad\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=100.0)\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        self.update_slow_critic()\n",
        "\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.log('ActorCritic/policy_loss', policy_loss.item())\n",
        "            logger.log('ActorCritic/value_loss', value_loss.item())\n",
        "            logger.log('ActorCritic/entropy_loss', entropy_loss.item())\n",
        "            logger.log('ActorCritic/S', S.item())\n",
        "            logger.log('ActorCritic/norm_ratio', norm_ratio.item())\n",
        "            logger.log('ActorCritic/total_loss', loss.item())"
      ],
      "metadata": {
        "id": "JEfUpJE9w6h-"
      },
      "id": "JEfUpJE9w6h-",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0662612e-701b-41a2-8679-25ad03fef367",
      "metadata": {
        "id": "0662612e-701b-41a2-8679-25ad03fef367"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## World Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "hXe_t1i7udVB"
      },
      "id": "hXe_t1i7udVB"
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBN(nn.Module):\n",
        "    def __init__(self, in_channels, stem_channels, final_feature_width) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        backbone = []\n",
        "        # stem\n",
        "        backbone.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=stem_channels,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                bias=False\n",
        "            )\n",
        "        )\n",
        "        feature_width = 64//2\n",
        "        channels = stem_channels\n",
        "        backbone.append(nn.BatchNorm2d(stem_channels))\n",
        "        backbone.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # layers\n",
        "        while True:\n",
        "            backbone.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=channels,\n",
        "                    out_channels=channels*2,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=False\n",
        "                )\n",
        "            )\n",
        "            channels *= 2\n",
        "            feature_width //= 2\n",
        "            backbone.append(nn.BatchNorm2d(channels))\n",
        "            backbone.append(nn.ReLU(inplace=True))\n",
        "\n",
        "            if feature_width == final_feature_width:\n",
        "                break\n",
        "\n",
        "        self.backbone = nn.Sequential(*backbone)\n",
        "        self.last_channels = channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = rearrange(x, \"B L C H W -> (B L) C H W\")\n",
        "        x = self.backbone(x)\n",
        "        x = rearrange(x, \"(B L) C H W -> B L (C H W)\", B=batch_size)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hHtbvmUvuzUj"
      },
      "id": "hHtbvmUvuzUj",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ewkSFqM13fhU"
      },
      "id": "ewkSFqM13fhU"
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBN(nn.Module):\n",
        "    def __init__(self, stoch_dim, last_channels, original_in_channels, stem_channels, final_feature_width) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        backbone = []\n",
        "        # stem\n",
        "        backbone.append(nn.Linear(stoch_dim, last_channels*final_feature_width*final_feature_width, bias=False))\n",
        "        backbone.append(Rearrange('B L (C H W) -> (B L) C H W', C=last_channels, H=final_feature_width))\n",
        "        backbone.append(nn.BatchNorm2d(last_channels))\n",
        "        backbone.append(nn.ReLU(inplace=True))\n",
        "        # residual_layer\n",
        "        # backbone.append(ResidualStack(last_channels, 1, last_channels//4))\n",
        "        # layers\n",
        "        channels = last_channels\n",
        "        feat_width = final_feature_width\n",
        "        while True:\n",
        "            if channels == stem_channels:\n",
        "                break\n",
        "            backbone.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=channels,\n",
        "                    out_channels=channels//2,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=False\n",
        "                )\n",
        "            )\n",
        "            channels //= 2\n",
        "            feat_width *= 2\n",
        "            backbone.append(nn.BatchNorm2d(channels))\n",
        "            backbone.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        backbone.append(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=channels,\n",
        "                out_channels=original_in_channels,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            )\n",
        "        )\n",
        "        self.backbone = nn.Sequential(*backbone)\n",
        "\n",
        "    def forward(self, sample):\n",
        "        batch_size = sample.shape[0]\n",
        "        obs_hat = self.backbone(sample)\n",
        "        obs_hat = rearrange(obs_hat, \"(B L) C H W -> B L C H W\", B=batch_size)\n",
        "        return obs_hat"
      ],
      "metadata": {
        "id": "iMAskLE-3iUB"
      },
      "id": "iMAskLE-3iUB",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dist Head"
      ],
      "metadata": {
        "id": "Ff9vFDq7Ql3M"
      },
      "id": "Ff9vFDq7Ql3M"
    },
    {
      "cell_type": "code",
      "source": [
        "class DistHead(nn.Module):\n",
        "    '''\n",
        "    Dist: abbreviation of distribution\n",
        "    '''\n",
        "    def __init__(self, image_feat_dim, transformer_hidden_dim, stoch_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.stoch_dim = stoch_dim\n",
        "        self.post_head = nn.Linear(image_feat_dim, stoch_dim*stoch_dim)\n",
        "        self.prior_head = nn.Linear(transformer_hidden_dim, stoch_dim*stoch_dim)\n",
        "\n",
        "    def unimix(self, logits, mixing_ratio=0.01):\n",
        "        # uniform noise mixing\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        mixed_probs = mixing_ratio * torch.ones_like(probs) / self.stoch_dim + (1-mixing_ratio) * probs\n",
        "        logits = torch.log(mixed_probs)\n",
        "        return logits\n",
        "\n",
        "    def forward_post(self, x):\n",
        "        logits = self.post_head(x)\n",
        "        logits = rearrange(logits, \"B L (K C) -> B L K C\", K=self.stoch_dim)\n",
        "        logits = self.unimix(logits)\n",
        "        return logits\n",
        "\n",
        "    def forward_prior(self, x):\n",
        "        logits = self.prior_head(x)\n",
        "        logits = rearrange(logits, \"B L (K C) -> B L K C\", K=self.stoch_dim)\n",
        "        logits = self.unimix(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "DpipfEIQQpBQ"
      },
      "id": "DpipfEIQQpBQ",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Decoder"
      ],
      "metadata": {
        "id": "zTy4fntCUp4L"
      },
      "id": "zTy4fntCUp4L"
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardDecoder(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_size, transformer_hidden_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.head = nn.Linear(transformer_hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        feat = self.backbone(feat)\n",
        "        reward = self.head(feat)\n",
        "        return reward"
      ],
      "metadata": {
        "id": "Q4hvfbbhU13p"
      },
      "id": "Q4hvfbbhU13p",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Termination Decoder"
      ],
      "metadata": {
        "id": "1BLiBDXLWYuK"
      },
      "id": "1BLiBDXLWYuK"
    },
    {
      "cell_type": "code",
      "source": [
        "class TerminationDecoder(nn.Module):\n",
        "    def __init__(self,  embedding_size, transformer_hidden_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(transformer_hidden_dim, 1),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, feat):\n",
        "        feat = self.backbone(feat)\n",
        "        termination = self.head(feat)\n",
        "        termination = termination.squeeze(-1)  # remove last 1 dim\n",
        "        return termination"
      ],
      "metadata": {
        "id": "dEb1IP3dWdNv"
      },
      "id": "dEb1IP3dWdNv",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSELoss"
      ],
      "metadata": {
        "id": "GqGMS6jYXSZX"
      },
      "id": "GqGMS6jYXSZX"
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, obs_hat, obs):\n",
        "        loss = (obs_hat - obs)**2\n",
        "        loss = reduce(loss, \"B L C H W -> B L\", \"sum\")\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "W5RzYMN5XVg4"
      },
      "id": "W5RzYMN5XVg4",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CategoricalKLDivLossWithFreeBits\n"
      ],
      "metadata": {
        "id": "mxq2T6qQXpuF"
      },
      "id": "mxq2T6qQXpuF"
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalKLDivLossWithFreeBits(nn.Module):\n",
        "    def __init__(self, free_bits) -> None:\n",
        "        super().__init__()\n",
        "        self.free_bits = free_bits\n",
        "\n",
        "    def forward(self, p_logits, q_logits):\n",
        "        p_dist = OneHotCategorical(logits=p_logits)\n",
        "        q_dist = OneHotCategorical(logits=q_logits)\n",
        "        kl_div = torch.distributions.kl.kl_divergence(p_dist, q_dist)\n",
        "        kl_div = reduce(kl_div, \"B L D -> B L\", \"sum\")\n",
        "        kl_div = kl_div.mean()\n",
        "        real_kl_div = kl_div\n",
        "        kl_div = torch.max(torch.ones_like(kl_div)*self.free_bits, kl_div)\n",
        "        return kl_div, real_kl_div"
      ],
      "metadata": {
        "id": "96-WIrvJXwS_"
      },
      "id": "96-WIrvJXwS_",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### World Model"
      ],
      "metadata": {
        "id": "uQnU0ewhrkdE"
      },
      "id": "uQnU0ewhrkdE"
    },
    {
      "cell_type": "code",
      "source": [
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, in_channels, action_dim,\n",
        "                 transformer_max_length, transformer_hidden_dim, transformer_num_layers, transformer_num_heads):\n",
        "        super().__init__()\n",
        "        self.transformer_hidden_dim = transformer_hidden_dim\n",
        "        self.final_feature_width = 4\n",
        "        self.stoch_dim = 32\n",
        "        self.stoch_flattened_dim = self.stoch_dim*self.stoch_dim\n",
        "        self.use_amp = True\n",
        "        self.tensor_dtype = torch.bfloat16 if self.use_amp else torch.float32\n",
        "        self.imagine_batch_size = -1\n",
        "        self.imagine_batch_length = -1\n",
        "\n",
        "        self.encoder = EncoderBN(\n",
        "            in_channels=in_channels,\n",
        "            stem_channels=32,\n",
        "            final_feature_width=self.final_feature_width\n",
        "        )\n",
        "        self.storm_transformer = StochasticTransformerKVCache(\n",
        "            stoch_dim=self.stoch_flattened_dim,\n",
        "            action_dim=action_dim,\n",
        "            feat_dim=transformer_hidden_dim,\n",
        "            num_layers=transformer_num_layers,\n",
        "            num_heads=transformer_num_heads,\n",
        "            max_length=transformer_max_length,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.dist_head = DistHead(\n",
        "            image_feat_dim=self.encoder.last_channels*self.final_feature_width*self.final_feature_width,\n",
        "            transformer_hidden_dim=transformer_hidden_dim,\n",
        "            stoch_dim=self.stoch_dim\n",
        "        )\n",
        "        self.image_decoder = DecoderBN(\n",
        "            stoch_dim=self.stoch_flattened_dim,\n",
        "            last_channels=self.encoder.last_channels,\n",
        "            original_in_channels=in_channels,\n",
        "            stem_channels=32,\n",
        "            final_feature_width=self.final_feature_width\n",
        "        )\n",
        "        self.reward_decoder = RewardDecoder(\n",
        "            num_classes=255,\n",
        "            embedding_size=self.stoch_flattened_dim,\n",
        "            transformer_hidden_dim=transformer_hidden_dim\n",
        "        )\n",
        "        self.termination_decoder = TerminationDecoder(\n",
        "            embedding_size=self.stoch_flattened_dim,\n",
        "            transformer_hidden_dim=transformer_hidden_dim\n",
        "        )\n",
        "\n",
        "        self.mse_loss_func = MSELoss()\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.bce_with_logits_loss_func = nn.BCEWithLogitsLoss()\n",
        "        self.symlog_twohot_loss_func = SymLogTwoHotLoss(num_classes=255, lower_bound=-20, upper_bound=20)\n",
        "        self.categorical_kl_div_loss = CategoricalKLDivLossWithFreeBits(free_bits=1)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
        "\n",
        "    def encode_obs(self, obs):\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.use_amp):\n",
        "            embedding = self.encoder(obs) # (B, L, image_feat_dim)\n",
        "            post_logits = self.dist_head.forward_post(embedding) # (B, L, n_classes, stoch_dim)\n",
        "            sample = self.stright_throught_gradient(post_logits, sample_mode=\"random_sample\") # (B, L, n_classes, stoch_dim) # one-hot\n",
        "            flattened_sample = self.flatten_sample(sample) # (B, L, n_classes*stoch_dim)\n",
        "        return flattened_sample\n",
        "\n",
        "    def calc_last_dist_feat(self, latent, action):\n",
        "        # 事前分布のサンプルと、Transformerからの最後の出力得る\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.use_amp):\n",
        "            temporal_mask = get_subsequent_mask(latent) # (1, L, L) # 下三角行列\n",
        "            dist_feat = self.storm_transformer(latent, action, temporal_mask) # (B, L, n_heads*d_v)\n",
        "            last_dist_feat = dist_feat[:, -1:] # (B, 1, n_heads*d_v)\n",
        "            prior_logits = self.dist_head.forward_prior(last_dist_feat) # (B, 1, n_classes, stoch_dim)\n",
        "            prior_sample = self.stright_throught_gradient(prior_logits, sample_mode=\"random_sample\") # (B, 1, n_classes, stoch_dim) # one-hot\n",
        "            prior_flattened_sample = self.flatten_sample(prior_sample) # (B, 1, n_classes*stoch_dim)\n",
        "        return prior_flattened_sample, last_dist_feat # (B, 1, n_classes*stoch_dim) # (B, 1, n_heads*d_v)\n",
        "\n",
        "    def predict_next(self, last_flattened_sample, action, log_video=True):\n",
        "        # いろんな次状態予測\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.use_amp):\n",
        "            dist_feat = self.storm_transformer.forward_with_kv_cache(last_flattened_sample, action) # (B, 1, 512) # Transformerからの出力h\n",
        "            prior_logits = self.dist_head.forward_prior(dist_feat) # (B, 1, n_classes, stoch_dim)\n",
        "\n",
        "            # decoding\n",
        "            prior_sample = self.stright_throught_gradient(prior_logits, sample_mode=\"random_sample\") # (B, 1, n_classes, stoch_dim) # one-hot\n",
        "            prior_flattened_sample = self.flatten_sample(prior_sample) # (B, 1, n_classes*stoch_dim)\n",
        "            if log_video:\n",
        "                obs_hat = self.image_decoder(prior_flattened_sample)\n",
        "            else:\n",
        "                obs_hat = None\n",
        "            reward_hat = self.reward_decoder(dist_feat) # (B, 1, num_classes=255)\n",
        "            reward_hat = self.symlog_twohot_loss_func.decode(reward_hat) # (1024, 1)\n",
        "            termination_hat = self.termination_decoder(dist_feat) # (1024, 1)\n",
        "            termination_hat = termination_hat > 0\n",
        "\n",
        "        return obs_hat, reward_hat, termination_hat, prior_flattened_sample, dist_feat\n",
        "\n",
        "    def stright_throught_gradient(self, logits, sample_mode=\"random_sample\"):\n",
        "        dist = OneHotCategorical(logits=logits)\n",
        "        if sample_mode == \"random_sample\":\n",
        "            sample = dist.sample() + dist.probs - dist.probs.detach()\n",
        "        elif sample_mode == \"mode\":\n",
        "            sample = dist.mode\n",
        "        elif sample_mode == \"probs\":\n",
        "            sample = dist.probs\n",
        "        return sample\n",
        "\n",
        "    def flatten_sample(self, sample):\n",
        "        return rearrange(sample, \"B L K C -> B L (K C)\")\n",
        "\n",
        "    def init_imagine_buffer(self, imagine_batch_size, imagine_batch_length, dtype):\n",
        "        '''\n",
        "        This can slightly improve the efficiency of imagine_data\n",
        "        But may vary across different machines\n",
        "        '''\n",
        "        if self.imagine_batch_size != imagine_batch_size or self.imagine_batch_length != imagine_batch_length:\n",
        "            print(f\"init_imagine_buffer: {imagine_batch_size}x{imagine_batch_length}@{dtype}\") # 1024 * 16\n",
        "            self.imagine_batch_size = imagine_batch_size\n",
        "            self.imagine_batch_length = imagine_batch_length\n",
        "            latent_size = (imagine_batch_size, imagine_batch_length+1, self.stoch_flattened_dim)\n",
        "            hidden_size = (imagine_batch_size, imagine_batch_length+1, self.transformer_hidden_dim)\n",
        "            scalar_size = (imagine_batch_size, imagine_batch_length)\n",
        "            self.latent_buffer = torch.zeros(latent_size, dtype=dtype, device=\"cuda\")\n",
        "            self.hidden_buffer = torch.zeros(hidden_size, dtype=dtype, device=\"cuda\")\n",
        "            self.action_buffer = torch.zeros(scalar_size, dtype=dtype, device=\"cuda\")\n",
        "            self.reward_hat_buffer = torch.zeros(scalar_size, dtype=dtype, device=\"cuda\")\n",
        "            self.termination_hat_buffer = torch.zeros(scalar_size, dtype=dtype, device=\"cuda\")\n",
        "\n",
        "    def imagine_data(self, agent: ActorCriticAgent, sample_obs, sample_action,\n",
        "                     imagine_batch_size, imagine_batch_length, log_video, logger):\n",
        "        self.init_imagine_buffer(imagine_batch_size, imagine_batch_length, dtype=self.tensor_dtype)\n",
        "        obs_hat_list = []\n",
        "\n",
        "        self.storm_transformer.reset_kv_cache_list(imagine_batch_size, dtype=self.tensor_dtype)\n",
        "        # context imagineを開始する前の準備\n",
        "        context_latent = self.encode_obs(sample_obs) # (B, L, n_classes*stoch_dim)\n",
        "        for i in range(sample_obs.shape[1]):  # context_length is sample_obs.shape[1]\n",
        "            last_obs_hat, last_reward_hat, last_termination_hat, last_latent, last_dist_feat = self.predict_next(\n",
        "                context_latent[:, i:i+1],\n",
        "                sample_action[:, i:i+1],\n",
        "                log_video=log_video\n",
        "            )\n",
        "        self.latent_buffer[:, 0:1] = last_latent # (B, 1, n_classes*stoch_dim) # 事前分布のsample(one-hot)をflattenしたもの\n",
        "        self.hidden_buffer[:, 0:1] = last_dist_feat # (B, 1, 512) # Transformerからの出力h\n",
        "\n",
        "        # imagine\n",
        "        for i in range(imagine_batch_length):\n",
        "            action = agent.sample(torch.cat([self.latent_buffer[:, i:i+1], self.hidden_buffer[:, i:i+1]], dim=-1))\n",
        "            self.action_buffer[:, i:i+1] = action # action: (B, 1)\n",
        "\n",
        "            last_obs_hat, last_reward_hat, last_termination_hat, last_latent, last_dist_feat = self.predict_next(\n",
        "                self.latent_buffer[:, i:i+1], self.action_buffer[:, i:i+1], log_video=log_video)\n",
        "\n",
        "            self.latent_buffer[:, i+1:i+2] = last_latent\n",
        "            self.hidden_buffer[:, i+1:i+2] = last_dist_feat\n",
        "            self.reward_hat_buffer[:, i:i+1] = last_reward_hat\n",
        "            self.termination_hat_buffer[:, i:i+1] = last_termination_hat\n",
        "            if log_video:\n",
        "                obs_hat_list.append(last_obs_hat[::imagine_batch_size//16])  # uniform sample vec_env\n",
        "\n",
        "        if log_video:\n",
        "            logger.log(\"Imagine/predict_video\", torch.clamp(torch.cat(obs_hat_list, dim=1), 0, 1).cpu().float().detach().numpy())\n",
        "\n",
        "        return torch.cat([self.latent_buffer, self.hidden_buffer], dim=-1), self.action_buffer, self.reward_hat_buffer, self.termination_hat_buffer\n",
        "\n",
        "    def update(self, obs, action, reward, termination, logger=None):\n",
        "        self.train()\n",
        "        batch_size, batch_length = obs.shape[:2]\n",
        "\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.use_amp):\n",
        "            # encoding\n",
        "            embedding = self.encoder(obs) # (B, L, image_feat_dim) # image_feat_dimはCNN通した後のCHWをflattenした次元\n",
        "            post_logits = self.dist_head.forward_post(embedding) # (B, L, n_classes, stoch_dim)\n",
        "            sample = self.stright_throught_gradient(post_logits, sample_mode=\"random_sample\") # (B, L, n_classes, stoch_dim)\n",
        "            flattened_sample = self.flatten_sample(sample) # (B, L, n_classes*stoch_dim) # 事後分布のflattenされたサンプル\n",
        "\n",
        "            # decoding image\n",
        "            obs_hat = self.image_decoder(flattened_sample) # (B, L, C, H, W)\n",
        "\n",
        "            # transformer\n",
        "            temporal_mask = get_subsequent_mask_with_batch_length(batch_length, flattened_sample.device) # (1, L, L)\n",
        "            dist_feat = self.storm_transformer(flattened_sample, action, temporal_mask) # (B, L, n*d_v)\n",
        "            prior_logits = self.dist_head.forward_prior(dist_feat) # (B, L, n_classes, stoch_dim)\n",
        "            # decoding reward and termination with dist_feat\n",
        "            reward_hat = self.reward_decoder(dist_feat) # (B, L, num_classes)\n",
        "            termination_hat = self.termination_decoder(dist_feat) # (B, L, 1)\n",
        "\n",
        "            # env loss\n",
        "            reconstruction_loss = self.mse_loss_func(obs_hat, obs)\n",
        "            reward_loss = self.symlog_twohot_loss_func(reward_hat, reward)\n",
        "            termination_loss = self.bce_with_logits_loss_func(termination_hat, termination)\n",
        "            # dyn-rep loss\n",
        "            dynamics_loss, dynamics_real_kl_div = self.categorical_kl_div_loss(post_logits[:, 1:].detach(), prior_logits[:, :-1])\n",
        "            representation_loss, representation_real_kl_div = self.categorical_kl_div_loss(post_logits[:, 1:], prior_logits[:, :-1].detach())\n",
        "            total_loss = reconstruction_loss + reward_loss + termination_loss + 0.5*dynamics_loss + 0.1*representation_loss\n",
        "\n",
        "        # gradient descent\n",
        "        self.scaler.scale(total_loss).backward()\n",
        "        self.scaler.unscale_(self.optimizer)  # for clip grad\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1000.0)\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.log(\"WorldModel/reconstruction_loss\", reconstruction_loss.item())\n",
        "            logger.log(\"WorldModel/reward_loss\", reward_loss.item())\n",
        "            logger.log(\"WorldModel/termination_loss\", termination_loss.item())\n",
        "            logger.log(\"WorldModel/dynamics_loss\", dynamics_loss.item())\n",
        "            logger.log(\"WorldModel/dynamics_real_kl_div\", dynamics_real_kl_div.item())\n",
        "            logger.log(\"WorldModel/representation_loss\", representation_loss.item())\n",
        "            logger.log(\"WorldModel/representation_real_kl_div\", representation_real_kl_div.item())\n",
        "            logger.log(\"WorldModel/total_loss\", total_loss.item())"
      ],
      "metadata": {
        "id": "YsmyQ1Ivrr_1"
      },
      "id": "YsmyQ1Ivrr_1",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## その他の機能"
      ],
      "metadata": {
        "id": "Pfr2XYoI0RiX"
      },
      "id": "Pfr2XYoI0RiX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReplayBuffer"
      ],
      "metadata": {
        "id": "7_2QAyqu0Wj6"
      },
      "id": "7_2QAyqu0Wj6"
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, obs_shape, num_envs, max_length=int(1E6), warmup_length=50000, store_on_gpu=False) -> None:\n",
        "        self.store_on_gpu = store_on_gpu\n",
        "        if store_on_gpu:\n",
        "            self.obs_buffer = torch.empty((max_length//num_envs, num_envs, *obs_shape), dtype=torch.uint8, device=\"cuda\", requires_grad=False)\n",
        "            self.action_buffer = torch.empty((max_length//num_envs, num_envs), dtype=torch.uint8, device=\"cuda\", requires_grad=False)\n",
        "            self.reward_buffer = torch.empty((max_length//num_envs, num_envs), dtype=torch.float32, device=\"cuda\", requires_grad=False)\n",
        "            self.termination_buffer = torch.empty((max_length//num_envs, num_envs), dtype=torch.float32, device=\"cuda\", requires_grad=False)\n",
        "        else:\n",
        "            self.obs_buffer = np.empty((max_length//num_envs, num_envs, *obs_shape), dtype=np.uint8)\n",
        "            self.action_buffer = np.empty((max_length//num_envs, num_envs), dtype=np.float32)\n",
        "            self.reward_buffer = np.empty((max_length//num_envs, num_envs), dtype=np.float32)\n",
        "            self.termination_buffer = np.empty((max_length//num_envs, num_envs), dtype=np.float32)\n",
        "\n",
        "        self.length = 0\n",
        "        self.num_envs = num_envs\n",
        "        self.last_pointer = -1\n",
        "        self.max_length = max_length\n",
        "        self.warmup_length = warmup_length\n",
        "        self.external_buffer_length = None\n",
        "\n",
        "    def load_trajectory(self, path):\n",
        "        buffer = pickle.load(open(path, \"rb\"))\n",
        "        if self.store_on_gpu:\n",
        "            self.external_buffer = {name: torch.from_numpy(buffer[name]).to(\"cuda\") for name in buffer}\n",
        "        else:\n",
        "            self.external_buffer = buffer\n",
        "        self.external_buffer_length = self.external_buffer[\"obs\"].shape[0]\n",
        "\n",
        "    def sample_external(self, batch_size, batch_length, to_device=\"cuda\"):\n",
        "        indexes = np.random.randint(0, self.external_buffer_length+1-batch_length, size=batch_size)\n",
        "        if self.store_on_gpu:\n",
        "            obs = torch.stack([self.external_buffer[\"obs\"][idx:idx+batch_length] for idx in indexes])\n",
        "            action = torch.stack([self.external_buffer[\"action\"][idx:idx+batch_length] for idx in indexes])\n",
        "            reward = torch.stack([self.external_buffer[\"reward\"][idx:idx+batch_length] for idx in indexes])\n",
        "            termination = torch.stack([self.external_buffer[\"done\"][idx:idx+batch_length] for idx in indexes])\n",
        "        else:\n",
        "            obs = np.stack([self.external_buffer[\"obs\"][idx:idx+batch_length] for idx in indexes])\n",
        "            action = np.stack([self.external_buffer[\"action\"][idx:idx+batch_length] for idx in indexes])\n",
        "            reward = np.stack([self.external_buffer[\"reward\"][idx:idx+batch_length] for idx in indexes])\n",
        "            termination = np.stack([self.external_buffer[\"done\"][idx:idx+batch_length] for idx in indexes])\n",
        "        return obs, action, reward, termination\n",
        "\n",
        "    def ready(self):\n",
        "        return self.length * self.num_envs > self.warmup_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size, external_batch_size, batch_length, to_device=\"cuda\"):\n",
        "        if self.store_on_gpu:\n",
        "            obs, action, reward, termination = [], [], [], []\n",
        "            if batch_size > 0:\n",
        "                for i in range(self.num_envs):\n",
        "                    indexes = np.random.randint(0, self.length+1-batch_length, size=batch_size//self.num_envs)\n",
        "                    obs.append(torch.stack([self.obs_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    action.append(torch.stack([self.action_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    reward.append(torch.stack([self.reward_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    termination.append(torch.stack([self.termination_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "\n",
        "            if self.external_buffer_length is not None and external_batch_size > 0:\n",
        "                external_obs, external_action, external_reward, external_termination = self.sample_external(\n",
        "                    external_batch_size, batch_length, to_device)\n",
        "                obs.append(external_obs)\n",
        "                action.append(external_action)\n",
        "                reward.append(external_reward)\n",
        "                termination.append(external_termination)\n",
        "\n",
        "            obs = torch.cat(obs, dim=0).float() / 255\n",
        "            obs = rearrange(obs, \"B T H W C -> B T C H W\") # Bのところは実際にはbatch_length * num_envsの長さかな？\n",
        "            action = torch.cat(action, dim=0)\n",
        "            reward = torch.cat(reward, dim=0)\n",
        "            termination = torch.cat(termination, dim=0)\n",
        "        else:\n",
        "            obs, action, reward, termination = [], [], [], []\n",
        "            if batch_size > 0:\n",
        "                for i in range(self.num_envs):\n",
        "                    indexes = np.random.randint(0, self.length+1-batch_length, size=batch_size//self.num_envs)\n",
        "                    obs.append(np.stack([self.obs_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    action.append(np.stack([self.action_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    reward.append(np.stack([self.reward_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    termination.append(np.stack([self.termination_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "\n",
        "            if self.external_buffer_length is not None and external_batch_size > 0:\n",
        "                external_obs, external_action, external_reward, external_termination = self.sample_external(\n",
        "                    external_batch_size, batch_length, to_device)\n",
        "                obs.append(external_obs)\n",
        "                action.append(external_action)\n",
        "                reward.append(external_reward)\n",
        "                termination.append(external_termination)\n",
        "\n",
        "            obs = torch.from_numpy(np.concatenate(obs, axis=0)).float().cuda() / 255\n",
        "            obs = rearrange(obs, \"B T H W C -> B T C H W\")\n",
        "            action = torch.from_numpy(np.concatenate(action, axis=0)).cuda()\n",
        "            reward = torch.from_numpy(np.concatenate(reward, axis=0)).cuda()\n",
        "            termination = torch.from_numpy(np.concatenate(termination, axis=0)).cuda()\n",
        "\n",
        "        return obs, action, reward, termination\n",
        "\n",
        "    def append(self, obs, action, reward, termination):\n",
        "        # obs/nex_obs: torch Tensor\n",
        "        # action/reward/termination: int or float or bool\n",
        "        self.last_pointer = (self.last_pointer + 1) % (self.max_length//self.num_envs)\n",
        "        if self.store_on_gpu:\n",
        "            self.obs_buffer[self.last_pointer] = torch.from_numpy(obs)\n",
        "            self.action_buffer[self.last_pointer] = torch.tensor(action, dtype=torch.uint8)\n",
        "            self.reward_buffer[self.last_pointer] = torch.tensor(reward, dtype=torch.float32)\n",
        "            self.termination_buffer[self.last_pointer] = torch.from_numpy(np.array(termination))\n",
        "        else:\n",
        "            self.obs_buffer[self.last_pointer] = obs\n",
        "            self.action_buffer[self.last_pointer] = action\n",
        "            self.reward_buffer[self.last_pointer] = reward\n",
        "            self.termination_buffer[self.last_pointer] = termination\n",
        "\n",
        "        if len(self) < self.max_length:\n",
        "            self.length += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length * self.num_envs"
      ],
      "metadata": {
        "id": "dY0Ap8My0WEg"
      },
      "id": "dY0Ap8My0WEg",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "hAHg6gee2lFs"
      },
      "id": "hAHg6gee2lFs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EMA Scalar"
      ],
      "metadata": {
        "id": "xoqJKDoK2o-C"
      },
      "id": "xoqJKDoK2o-C"
    },
    {
      "cell_type": "code",
      "source": [
        "class EMAScalar():\n",
        "    def __init__(self, decay) -> None:\n",
        "        self.scalar = 0.0\n",
        "        self.decay = decay\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.get()\n",
        "\n",
        "    def update(self, value):\n",
        "        self.scalar = self.scalar * self.decay + value * (1 - self.decay)\n",
        "\n",
        "    def get(self):\n",
        "        return self.scalar"
      ],
      "metadata": {
        "id": "LoMOv98p2reN"
      },
      "id": "LoMOv98p2reN",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logger"
      ],
      "metadata": {
        "id": "8z9QXtbhueHt"
      },
      "id": "8z9QXtbhueHt"
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger():\n",
        "    def __init__(self, path) -> None:\n",
        "        self.writer = SummaryWriter(path, flush_secs=1)\n",
        "        self.tag_step = {}\n",
        "\n",
        "    def log(self, tag, value):\n",
        "        if tag not in self.tag_step:\n",
        "            self.tag_step[tag] = 0\n",
        "        else:\n",
        "            self.tag_step[tag] += 1\n",
        "        if \"video\" in tag:\n",
        "            self.writer.add_video(tag, value, self.tag_step[tag], fps=15)\n",
        "        elif \"images\" in tag:\n",
        "            self.writer.add_images(tag, value, self.tag_step[tag])\n",
        "        elif \"hist\" in tag:\n",
        "            self.writer.add_histogram(tag, value, self.tag_step[tag])\n",
        "        else:\n",
        "            self.writer.add_scalar(tag, value, self.tag_step[tag])"
      ],
      "metadata": {
        "id": "2dH1CeESugAT"
      },
      "id": "2dH1CeESugAT",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b06c188f-8a87-42e7-9f61-7f385eccc565",
      "metadata": {
        "id": "b06c188f-8a87-42e7-9f61-7f385eccc565"
      },
      "source": [
        "## 5. 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "DNhFa6PakHO8"
      },
      "id": "DNhFa6PakHO8"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "48a14cd1-dc80-4f22-a199-131fa267e14c",
      "metadata": {
        "id": "48a14cd1-dc80-4f22-a199-131fa267e14c"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # basic settings\n",
        "        self.seed = 0\n",
        "        self.ImageSize = 64\n",
        "        self.ReplayBufferOnGPU = False\n",
        "\n",
        "        # joint train agent\n",
        "        self.SampleMaxSteps = 102000\n",
        "        self.BufferMaxLength = 100000\n",
        "        self.BufferWarmUp = 1024\n",
        "        self.NumEnvs = 1\n",
        "        self.BatchSize = 16\n",
        "        self.DemonstrationBatchSize = 4\n",
        "        self.BatchLength = 64\n",
        "        self.ImagineBatchSize = 1024\n",
        "        self.ImagineDemonstrationBatchSize = 256\n",
        "        self.ImagineContextLength = 8\n",
        "        self.ImagineBatchLength = 16\n",
        "        self.TrainDynamicsEverySteps = 1\n",
        "        self.TrainAgentEverySteps = 1\n",
        "        self.UseDemonstration = False\n",
        "        self.SaveEverySteps = 2500\n",
        "\n",
        "        # world models\n",
        "        self.WorldModel_InChannels = 3\n",
        "        self.WorldModel_TransformerMaxLength = 64\n",
        "        self.WorldModel_TransformerHiddenDim = 512\n",
        "        self.WorldModel_TransformerNumLayers = 2\n",
        "        self.WorldModel_TransformerNumHeads = 8\n",
        "\n",
        "        # Agent\n",
        "        self.Agent_NumLayers = 2\n",
        "        self.Agent_HiddenDim =  512\n",
        "        self.Agent_Gamma = 0.985\n",
        "        self.Agent_Lambda = 0.95\n",
        "        self.Agent_EntropyCoef = 3E-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make env"
      ],
      "metadata": {
        "id": "DSpsoYnSsuSE"
      },
      "id": "DSpsoYnSsuSE"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(seed=None, img_size=64, max_steps=100_000):\n",
        "    env = gym.make(\"ALE/MsPacman-v5\")\n",
        "\n",
        "    # シード固定\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    env = ResizeObservation(env, (img_size, img_size))\n",
        "    env = RepeatAction(env=env, skip=4, max_steps=max_steps)\n",
        "\n",
        "    return env"
      ],
      "metadata": {
        "id": "8aKSET75swOX"
      },
      "id": "8aKSET75swOX",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build vec env"
      ],
      "metadata": {
        "id": "KqaPl02bml0C"
      },
      "id": "KqaPl02bml0C"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vec_env(env_name, image_size, num_envs, seed):\n",
        "    # lambda pitfall refs to: https://python.plainenglish.io/python-pitfalls-with-variable-capture-dcfc113f39b7\n",
        "    def lambda_generator(env_name, image_size):\n",
        "        return lambda: make_env(seed=seed)\n",
        "    env_fns = []\n",
        "    env_fns = [lambda_generator(env_name, image_size) for i in range(num_envs)]\n",
        "    vec_env = gym.vector.AsyncVectorEnv(env_fns=env_fns)\n",
        "    return vec_env"
      ],
      "metadata": {
        "id": "Hsu8hh73sbxB"
      },
      "id": "Hsu8hh73sbxB",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train World Models Step"
      ],
      "metadata": {
        "id": "gA76wQz6uIus"
      },
      "id": "gA76wQz6uIus"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_world_model_step(replay_buffer: ReplayBuffer, world_model: WorldModel, batch_size, demonstration_batch_size, batch_length, logger):\n",
        "    obs, action, reward, termination = replay_buffer.sample(batch_size, demonstration_batch_size, batch_length)\n",
        "    world_model.update(obs, action, reward, termination, logger=logger)"
      ],
      "metadata": {
        "id": "eiaEWTcMuOMI"
      },
      "id": "eiaEWTcMuOMI",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### World Model Imagine Data"
      ],
      "metadata": {
        "id": "h4qgJf40gtzw"
      },
      "id": "h4qgJf40gtzw"
    },
    {
      "cell_type": "code",
      "source": [
        "def world_model_imagine_data(replay_buffer: ReplayBuffer,\n",
        "                             world_model: WorldModel, agent: ActorCriticAgent,\n",
        "                             imagine_batch_size, imagine_demonstration_batch_size,\n",
        "                             imagine_context_length, imagine_batch_length,\n",
        "                             log_video, logger):\n",
        "    '''\n",
        "    Sample context from replay buffer, then imagine data with world model and agent\n",
        "    '''\n",
        "    world_model.eval()\n",
        "    agent.eval()\n",
        "\n",
        "    sample_obs, sample_action, sample_reward, sample_termination = replay_buffer.sample(\n",
        "        imagine_batch_size, imagine_demonstration_batch_size, imagine_context_length)\n",
        "    # sample_obs: (B=1024, context=8, W, H, C)\n",
        "\n",
        "    latent, action, reward_hat, termination_hat = world_model.imagine_data(\n",
        "        agent, sample_obs, sample_action,\n",
        "        imagine_batch_size=imagine_batch_size+imagine_demonstration_batch_size,\n",
        "        imagine_batch_length=imagine_batch_length,\n",
        "        log_video=log_video,\n",
        "        logger=logger\n",
        "    )\n",
        "    # latent shape torch.Size([1024, 17, 1536]) 1536は32*32+transformer hidden_dim 17はimagine_length+1\n",
        "    # action shape torch.Size([1024, 16])\n",
        "    # reward_hat shape torch.Size([1024, 16])\n",
        "    # termination_hat shape torch.Size([1024, 16])\n",
        "    return latent, action, None, None, reward_hat, termination_hat"
      ],
      "metadata": {
        "id": "WB3rsvzSgwNt"
      },
      "id": "WB3rsvzSgwNt",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joint Train World Model Agent"
      ],
      "metadata": {
        "id": "QEboY8pIW9B5"
      },
      "id": "QEboY8pIW9B5"
    },
    {
      "cell_type": "code",
      "source": [
        "def joint_train_world_model_agent(env_name, max_steps, num_envs, image_size,\n",
        "                                  replay_buffer: ReplayBuffer,\n",
        "                                  world_model: WorldModel, agent: ActorCriticAgent,\n",
        "                                  train_dynamics_every_steps, train_agent_every_steps,\n",
        "                                  batch_size, demonstration_batch_size, batch_length,\n",
        "                                  imagine_batch_size, imagine_demonstration_batch_size,\n",
        "                                  imagine_context_length, imagine_batch_length,\n",
        "                                  save_every_steps, seed, logger):\n",
        "    # create ckpt dir\n",
        "    os.makedirs(f\"ckpt\", exist_ok=True)\n",
        "\n",
        "    # build vec env, not useful in the Atari100k setting\n",
        "    # but when the max_steps is large, you can use parallel envs to speed up\n",
        "    # vec_env = build_vec_env(env_name, image_size, num_envs=num_envs, seed=seed)\n",
        "    env = make_env(seed=seed)\n",
        "    # print(\"Current env: \" + colorama.Fore.YELLOW + f\"{env_name}\" + colorama.Style.RESET_ALL)\n",
        "\n",
        "    # reset envs and variables\n",
        "    sum_reward = 0\n",
        "    current_obs, current_info = env.reset() # (H, W, C)\n",
        "    context_obs = deque(maxlen=16)\n",
        "    context_action = deque(maxlen=16)\n",
        "\n",
        "    # sample and train\n",
        "    for total_steps in tqdm(range(max_steps//num_envs)):\n",
        "        # sample part >>>\n",
        "        if replay_buffer.ready():\n",
        "            world_model.eval()\n",
        "            agent.eval()\n",
        "            with torch.no_grad():\n",
        "                if len(context_action) == 0:\n",
        "                    action = env.action_space.sample() # (action_dim=1, )\n",
        "                else:\n",
        "                    context_latent = world_model.encode_obs(torch.cat(list(context_obs), dim=1)) # (B=1?, L, n_classes*stoch_dim)\n",
        "                    print('context_action', context_action)\n",
        "                    model_context_action = np.stack(list(context_action), axis=1) # (B, L)\n",
        "                    model_context_action = torch.Tensor(model_context_action).cuda()\n",
        "                    print('model_context_action shape', model_context_action.shape)\n",
        "                    prior_flattened_sample, last_dist_feat = world_model.calc_last_dist_feat(context_latent, model_context_action)\n",
        "                    print('prior_flattened_sample shape', prior_flattened_sample.shape)\n",
        "                    print('last_dist_feat shape', last_dist_feat.shape)\n",
        "                    action = agent.sample_as_env_action(\n",
        "                        torch.cat([prior_flattened_sample, last_dist_feat], dim=-1),\n",
        "                        greedy=False\n",
        "                    )\n",
        "\n",
        "            context_obs.append(rearrange(torch.Tensor(current_obs).cuda(), \"H W C -> 1 1 C H W\")/255)\n",
        "            context_action.append([action]) # []でB次元追加してる\n",
        "        else:\n",
        "            action = env.action_space.sample() # (action_dim=1, )\n",
        "\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        replay_buffer.append(current_obs, action, reward, np.logical_or(done, info[\"life_loss\"]))\n",
        "\n",
        "        if done:\n",
        "            logger.log(f\"sample/{env_name}_reward\", sum_reward)\n",
        "            logger.log(f\"sample/{env_name}_episode_steps\", current_info[\"episode_frame_number\"]//4)  # framskip=4\n",
        "            logger.log(\"replay_buffer/length\", len(replay_buffer))\n",
        "            sum_reward = 0\n",
        "\n",
        "        # update current_obs, current_info and sum_reward\n",
        "        sum_reward += reward\n",
        "        current_obs = obs\n",
        "        current_info = info\n",
        "        # <<< sample part\n",
        "\n",
        "        # train world model part >>>\n",
        "        if replay_buffer.ready() and total_steps % (train_dynamics_every_steps//num_envs) == 0:\n",
        "            train_world_model_step(\n",
        "                replay_buffer=replay_buffer,\n",
        "                world_model=world_model,\n",
        "                batch_size=batch_size,\n",
        "                demonstration_batch_size=demonstration_batch_size,\n",
        "                batch_length=batch_length,\n",
        "                logger=logger\n",
        "            )\n",
        "        # <<< train world model part\n",
        "\n",
        "        # train agent part >>>\n",
        "        if replay_buffer.ready() and total_steps % (train_agent_every_steps//num_envs) == 0 and total_steps*num_envs >= 0:\n",
        "            if total_steps % (save_every_steps//num_envs) == 0:\n",
        "                log_video = True\n",
        "            else:\n",
        "                log_video = False\n",
        "\n",
        "            imagine_latent, agent_action, agent_logprob, agent_value, imagine_reward, imagine_termination = world_model_imagine_data(\n",
        "                replay_buffer=replay_buffer,\n",
        "                world_model=world_model,\n",
        "                agent=agent,\n",
        "                imagine_batch_size=imagine_batch_size,\n",
        "                imagine_demonstration_batch_size=imagine_demonstration_batch_size,\n",
        "                imagine_context_length=imagine_context_length,\n",
        "                imagine_batch_length=imagine_batch_length,\n",
        "                log_video=log_video,\n",
        "                logger=logger\n",
        "            )\n",
        "            # imagine_latent shape torch.Size([1024, 17, 1536]) 1536は32*32+transformer hidden_dim 17はimagine_length+1\n",
        "            # agent_action shape torch.Size([1024, 16])\n",
        "            # None\n",
        "            # None\n",
        "            # imagine_reward shape torch.Size([1024, 16])\n",
        "            # imagine_termination shape torch.Size([1024, 16])\n",
        "\n",
        "            agent.update(\n",
        "                latent=imagine_latent.detach(),\n",
        "                action=agent_action.detach(),\n",
        "                old_logprob=agent_logprob,\n",
        "                old_value=agent_value,\n",
        "                reward=imagine_reward.detach(),\n",
        "                termination=imagine_termination.detach(),\n",
        "                logger=logger\n",
        "            )\n",
        "        # <<< train agent part\n",
        "\n",
        "        # save model per episode\n",
        "        if total_steps % (save_every_steps//num_envs) == 0:\n",
        "            print(f\"Saving model at total steps {total_steps}\")\n",
        "            torch.save(world_model.state_dict(), f\"ckpt/world_model_{total_steps}.pth\")\n",
        "            torch.save(agent.state_dict(), f\"ckpt/agent_{total_steps}.pth\")\n"
      ],
      "metadata": {
        "id": "STQmEFtfXA3X"
      },
      "id": "STQmEFtfXA3X",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build World Model"
      ],
      "metadata": {
        "id": "rN_nxN0-3kvh"
      },
      "id": "rN_nxN0-3kvh"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_world_model(config, action_dim):\n",
        "    return WorldModel(\n",
        "        in_channels=config.WorldModel_InChannels,\n",
        "        action_dim=action_dim,\n",
        "        transformer_max_length=config.WorldModel_TransformerMaxLength,\n",
        "        transformer_hidden_dim=config.WorldModel_TransformerHiddenDim,\n",
        "        transformer_num_layers=config.WorldModel_TransformerNumLayers,\n",
        "        transformer_num_heads=config.WorldModel_TransformerNumHeads\n",
        "    ).cuda()"
      ],
      "metadata": {
        "id": "EXLb82wr3n6T"
      },
      "id": "EXLb82wr3n6T",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Agent"
      ],
      "metadata": {
        "id": "oQ6ILwbITU_G"
      },
      "id": "oQ6ILwbITU_G"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(conf, action_dim):\n",
        "    return ActorCriticAgent(\n",
        "        feat_dim=32*32+config.WorldModel_TransformerHiddenDim,\n",
        "        num_layers=config.Agent_NumLayers,\n",
        "        hidden_dim=config.Agent_HiddenDim,\n",
        "        action_dim=action_dim,\n",
        "        gamma=config.Agent_Gamma,\n",
        "        lambd=config.Agent_Lambda,\n",
        "        entropy_coef=config.Agent_EntropyCoef,\n",
        "    ).cuda()"
      ],
      "metadata": {
        "id": "bm55cmpGUPpU"
      },
      "id": "bm55cmpGUPpU",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実際の学習"
      ],
      "metadata": {
        "id": "j2ubSwVsvA8O"
      },
      "id": "j2ubSwVsvA8O"
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# set seed\n",
        "set_seed()\n",
        "# tensorboard writer\n",
        "logger = Logger(path=\"./logs\")\n",
        "\n",
        "# getting action_dim with dummy env\n",
        "dummy_env = make_env(seed=0)\n",
        "action_dim = dummy_env.action_space.n\n",
        "\n",
        "# build world model and agent\n",
        "world_model = build_world_model(config, action_dim)\n",
        "agent = build_agent(config, action_dim)\n",
        "\n",
        "# build replay buffer\n",
        "replay_buffer = ReplayBuffer(\n",
        "    obs_shape=(config.ImageSize, config.ImageSize, 3),\n",
        "    num_envs=config.NumEnvs,\n",
        "    max_length=config.BufferMaxLength,\n",
        "    warmup_length=config.BufferWarmUp,\n",
        "    store_on_gpu=config.ReplayBufferOnGPU\n",
        ")\n",
        "\n",
        "# # judge whether to load demonstration trajectory\n",
        "# if config.UseDemonstration:\n",
        "#     print(colorama.Fore.MAGENTA + f\"loading demonstration trajectory from {args.trajectory_path}\" + colorama.Style.RESET_ALL)\n",
        "#     replay_buffer.load_trajectory(path=args.trajectory_path)\n",
        "\n",
        "# train\n",
        "joint_train_world_model_agent(\n",
        "    env_name='PacMan',\n",
        "    num_envs=config.NumEnvs,\n",
        "    max_steps=config.SampleMaxSteps,\n",
        "    image_size=config.ImageSize,\n",
        "    replay_buffer=replay_buffer,\n",
        "    world_model=world_model,\n",
        "    agent=agent,\n",
        "    train_dynamics_every_steps=config.TrainDynamicsEverySteps,\n",
        "    train_agent_every_steps=config.TrainAgentEverySteps,\n",
        "    batch_size=config.BatchSize,\n",
        "    demonstration_batch_size=config.DemonstrationBatchSize if config.UseDemonstration else 0,\n",
        "    batch_length=config.BatchLength,\n",
        "    imagine_batch_size=config.ImagineBatchSize,\n",
        "    imagine_demonstration_batch_size=config.ImagineDemonstrationBatchSize if config.UseDemonstration else 0,\n",
        "    imagine_context_length=config.ImagineContextLength,\n",
        "    imagine_batch_length=config.ImagineBatchLength,\n",
        "    save_every_steps=config.SaveEverySteps,\n",
        "    seed=config.seed,\n",
        "    logger=logger\n",
        ")\n"
      ],
      "metadata": {
        "id": "iF__tf3VvLg5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "a22c5288-a6be-4fc7-d6b2-8abfc6523cc0"
      },
      "id": "iF__tf3VvLg5",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/102000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at total steps 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 993/102000 [00:03<02:24, 699.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init_imagine_buffer: 1024x16@torch.bfloat16\n",
            "context_action deque([[7]], maxlen=16)\n",
            "model_context_action shape torch.Size([1, 1])\n",
            "prior_flattened_sample shape torch.Size([1, 1, 1024])\n",
            "last_dist_feat shape torch.Size([1, 1, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1026/102000 [00:25<41:38, 40.41it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 7282 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 13.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-80e4c15a4689>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m joint_train_world_model_agent(\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PacMan'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumEnvs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-dfda1eb08091>\u001b[0m in \u001b[0;36mjoint_train_world_model_agent\u001b[0;34m(env_name, max_steps, num_envs, image_size, replay_buffer, world_model, agent, train_dynamics_every_steps, train_agent_every_steps, batch_size, demonstration_batch_size, batch_length, imagine_batch_size, imagine_demonstration_batch_size, imagine_context_length, imagine_batch_length, save_every_steps, seed, logger)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mlog_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             imagine_latent, agent_action, agent_logprob, agent_value, imagine_reward, imagine_termination = world_model_imagine_data(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mworld_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworld_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-02bb0f82e720>\u001b[0m in \u001b[0;36mworld_model_imagine_data\u001b[0;34m(replay_buffer, world_model, agent, imagine_batch_size, imagine_demonstration_batch_size, imagine_context_length, imagine_batch_length, log_video, logger)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# sample_obs: (B=1024, context=8, W, H, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     latent, action, reward_hat, termination_hat = world_model.imagine_data(\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimagine_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimagine_batch_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimagine_demonstration_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-31be50953c08>\u001b[0m in \u001b[0;36mimagine_data\u001b[0;34m(self, agent, sample_obs, sample_action, imagine_batch_size, imagine_batch_length, log_video, logger)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mcontext_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, L, n_classes*stoch_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# context_length is sample_obs.shape[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             last_obs_hat, last_reward_hat, last_termination_hat, last_latent, last_dist_feat = self.predict_next(\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mcontext_latent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0msample_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-31be50953c08>\u001b[0m in \u001b[0;36mpredict_next\u001b[0;34m(self, last_flattened_sample, action, log_video)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# いろんな次状態予測\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mdist_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorm_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_with_kv_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_flattened_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, 1, 512) # Transformerからの出力h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mprior_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_feat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, 1, n_classes, stoch_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-590c2ed38009>\u001b[0m in \u001b[0;36mforward_with_kv_cache\u001b[0;34m(self, samples, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-b83ffecc6d7b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, slf_attn_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslf_attn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslf_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslf_attn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-83cfb5f06ee1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_qs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_ks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_vs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Transpose for attention dot product: b x n x lq x dv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 7282 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 13.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in agent.named_parameters():\n",
        "    print(f\"Parameter name: {name}\")\n",
        "    if param.grad is not None:\n",
        "        print(f\"Gradient shape: {param.grad.shape}\")\n",
        "        print(f\"Gradient values (first 10 elements):\\n{param.grad.view(-1)[:10]}\")  # 最初の一部を表示\n",
        "        # print(f\"Gradient values:\\n{param.grad}\")  # 全体を表示（大きい場合は注意）\n",
        "    else:\n",
        "        print(\"Gradient: None\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4mpaUFME8dr",
        "outputId": "7fd22977-a4d4-470e-832a-585eb5751a5c"
      },
      "id": "h4mpaUFME8dr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: actor.0.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.1.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.1.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.3.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.4.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.4.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.6.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: actor.6.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.0.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.1.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.1.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.3.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.4.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.4.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.6.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: critic.6.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.0.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.1.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.1.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.3.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.4.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.4.bias\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.6.weight\n",
            "Gradient: None\n",
            "--------------------\n",
            "Parameter name: slow_critic.6.bias\n",
            "Gradient: None\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rHs1TS4rsDjp"
      },
      "id": "rHs1TS4rsDjp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64e973f-6c7d-48ea-8ff5-aca31361067f",
      "metadata": {
        "id": "d64e973f-6c7d-48ea-8ff5-aca31361067f"
      },
      "outputs": [],
      "source": [
        "# モデル等の初期化\n",
        "seed = 0\n",
        "NUM_ITER = 100_000  # 環境とのインタラクション回数の制限 ※変更しないでください\n",
        "set_seed(seed)\n",
        "env = make_env(max_steps=NUM_ITER)\n",
        "eval_env = make_env(seed=1234, max_steps=None)  # omnicampus上の環境と同じシード値で評価環境を作成\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "# リプレイバッファ\n",
        "replay_buffer = ReplayBuffer(\n",
        "    capacity=cfg.buffer_size,\n",
        "    observation_shape=(64, 64, 1),\n",
        "    action_dim=env.action_space.n\n",
        ")\n",
        "\n",
        "# モデル\n",
        "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "target_critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    rssm,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    reward_model,\n",
        "    discount_model,\n",
        "    actor,\n",
        "    critic\n",
        ")\n",
        "\n",
        "# optimizer\n",
        "wm_params = list(rssm.parameters())         + \\\n",
        "            list(encoder.parameters())      + \\\n",
        "            list(decoder.parameters())      + \\\n",
        "            list(reward_model.parameters()) + \\\n",
        "            list(discount_model.parameters())\n",
        "\n",
        "wm_optimizer = torch.optim.AdamW(wm_params, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "actor_optimizer = torch.optim.AdamW(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa8b872c-8b0f-451b-b8ac-da70edecd2f7",
      "metadata": {
        "id": "fa8b872c-8b0f-451b-b8ac-da70edecd2f7"
      },
      "outputs": [],
      "source": [
        "def evaluation(eval_env: RepeatAction, policy: Agent, step: int, cfg: Config):\n",
        "    \"\"\"\n",
        "    評価用の関数．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    policy : Agent\n",
        "        エージェントのインスタンス．\n",
        "    step : int\n",
        "        現状の訓練のステップ数．\n",
        "    cfg : Config\n",
        "        コンフィグ．\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    max_ep_rewards : float\n",
        "        評価中に1エピソードで得た最大の報酬和．\n",
        "    \"\"\"\n",
        "    env = eval_env\n",
        "    all_ep_rewards = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(cfg.eval_episodes):\n",
        "            obs = env.reset()  # 環境をリセット\n",
        "            policy.reset()  # RNNの隠れ状態をリセット\n",
        "            done = False  # 終端条件\n",
        "            episode_reward = 0  # エピソードでの報酬和\n",
        "            while not done:\n",
        "                action = policy(obs, eval=True)\n",
        "                action_int = np.argmax(action)\n",
        "\n",
        "                obs, reward, done, _ = env.step(action_int)\n",
        "                episode_reward += reward\n",
        "\n",
        "            all_ep_rewards.append(episode_reward)\n",
        "\n",
        "        mean_ep_rewards = np.mean(all_ep_rewards)\n",
        "        max_ep_rewards = np.max(all_ep_rewards)\n",
        "        print(f\"Eval(iter={step}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n",
        "\n",
        "    return max_ep_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a3a0b3-bbe9-4f8f-8ae6-05720a9007b3",
      "metadata": {
        "id": "55a3a0b3-bbe9-4f8f-8ae6-05720a9007b3"
      },
      "outputs": [],
      "source": [
        "# ランダム行動でバッファを埋める\n",
        "obs = env.reset()\n",
        "done = False\n",
        "for _ in range(cfg.seed_iter):\n",
        "    action = env.action_space.sample()\n",
        "    next_obs, reward, done, _ = env.step(action)\n",
        "    action = F.one_hot(torch.tensor(action), num_classes=env.action_space.n)\n",
        "\n",
        "    if done:\n",
        "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "    else:\n",
        "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)\n",
        "        obs = next_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b35d8f-5339-42c4-90a3-581d17a3960d",
      "metadata": {
        "id": "a8b35d8f-5339-42c4-90a3-581d17a3960d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 学習を行う\n",
        "# 環境と相互作用 → 一定イテレーションでモデル更新を繰り返す\n",
        "policy = Agent(encoder, rssm, actor)\n",
        "\n",
        "# 環境，収益等の初期化\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "total_episode = 1\n",
        "best_reward = -1\n",
        "\n",
        "for iteration in range(NUM_ITER - cfg.seed_iter):\n",
        "    with torch.no_grad():\n",
        "        # 環境と相互作用\n",
        "        action = policy(obs)  # モデルで行動をサンプリング(one-hot)\n",
        "        action_int = np.argmax(action)  # 環境に渡すときはint型\n",
        "        next_obs, reward, done, _ = env.step(action_int)  # 環境を進める\n",
        "\n",
        "        # 得たデータをリプレイバッファに追加して更新\n",
        "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)  # x_t, a_t, r_t, gamma_t\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "    if (iteration + 1) % cfg.update_freq == 0:\n",
        "        # モデルの学習\n",
        "        # リプレイバッファからデータをサンプリングする\n",
        "        # (batch size, seq_lenght, *data shape)\n",
        "        observations, actions, rewards, done_flags =\\\n",
        "            replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n",
        "        done_flags = 1 - done_flags  # 終端でない場合に1をとる\n",
        "\n",
        "        # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n",
        "        observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n",
        "        actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n",
        "        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n",
        "        done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n",
        "\n",
        "        # =================\n",
        "        # world modelの学習\n",
        "        # =================\n",
        "        # 観測をベクトルに埋めこみ\n",
        "        emb_observations = encoder(observations.reshape(-1, 1, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n",
        "\n",
        "        # 状態表現z，行動aはゼロで初期化\n",
        "        # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n",
        "        state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
        "        rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
        "\n",
        "        # 各観測に対して状態表現を計算\n",
        "        # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n",
        "        states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
        "        rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
        "\n",
        "        # prior, posteriorを計算してKL lossを計算する\n",
        "        kl_loss = 0\n",
        "        for i in range(cfg.seq_length-1):\n",
        "            # rnn hiddenを更新\n",
        "            rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n",
        "\n",
        "            # prior, posteriorを計算\n",
        "            next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True) # \\hat{z}_t+1\n",
        "            next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)  # z_t+1\n",
        "\n",
        "            # posteriorからzをサンプリング\n",
        "            state = next_state_posterior.rsample().flatten(1)\n",
        "            rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n",
        "            states[i+1] = state  # z_t+1\n",
        "\n",
        "            # KL lossを計算\n",
        "            kl_loss +=  cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior)) + \\\n",
        "                        (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n",
        "        kl_loss /= (cfg.seq_length - 1)\n",
        "\n",
        "        # 初期状態は使わない\n",
        "        rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n",
        "        states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n",
        "\n",
        "        # 得られた状態を利用して再構成，報酬，終端フラグを予測\n",
        "        # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n",
        "        flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n",
        "        flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n",
        "\n",
        "        # 上から再構成，報酬，終端フラグ予測\n",
        "        obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n",
        "        reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n",
        "        discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n",
        "\n",
        "        # 各予測に対する損失の計算（対数尤度）\n",
        "        C, H, W = observations.shape[2:]\n",
        "        obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n",
        "        reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n",
        "        discount_loss = -torch.mean(discount_dist.log_prob(done_flags[:-1].float().reshape(-1, 1)))\n",
        "\n",
        "        # 総和をとってモデルを更新\n",
        "        wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n",
        "\n",
        "        wm_optimizer.zero_grad()\n",
        "        wm_loss.backward()\n",
        "        clip_grad_norm_(wm_params, cfg.gradient_clipping)\n",
        "        wm_optimizer.step()\n",
        "\n",
        "        #====================\n",
        "        # Actor, Criticの更新\n",
        "        #===================\n",
        "        # wmから得た状態の勾配を切っておく\n",
        "        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n",
        "        flatten_states = flatten_states.detach()\n",
        "\n",
        "        # priorを用いた状態予測\n",
        "        # 格納する空のTensorを用意\n",
        "        imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n",
        "                                      *flatten_states.shape,\n",
        "                                      device=flatten_states.device)\n",
        "        imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n",
        "                                           *flatten_rnn_hiddens.shape,\n",
        "                                           device=flatten_rnn_hiddens.device)\n",
        "        imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n",
        "                                                device=flatten_rnn_hiddens.device)\n",
        "        imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n",
        "                                                device=flatten_rnn_hiddens.device)\n",
        "\n",
        "        # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n",
        "        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n",
        "        imagined_states[0] = flatten_states\n",
        "        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n",
        "\n",
        "        # open-loopで予測\n",
        "        for i in range(1, cfg.imagination_horizon + 1):\n",
        "            actions, action_log_probs, action_entropys = actor(flatten_states.detach(), flatten_rnn_hiddens.detach())  # ((T-1) * B, action dim)\n",
        "\n",
        "            # rnn hiddenを更新, priorで次の状態を予測\n",
        "            with torch.no_grad():\n",
        "                flatten_rnn_hiddens = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n",
        "                flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n",
        "                flatten_states = flatten_states_prior.rsample().flatten(1)\n",
        "\n",
        "            imagined_rnn_hiddens[i] = flatten_rnn_hiddens.detach()\n",
        "            imagined_states[i] = flatten_states.detach()\n",
        "            imagined_action_log_probs[i-1] = action_log_probs\n",
        "            imagined_action_entropys[i-1] = action_entropys\n",
        "\n",
        "        imagined_states = imagined_states[1:]\n",
        "        imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n",
        "\n",
        "        # 得られた状態から報酬を予測\n",
        "        flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes).detach()  # ((imagination horizon) * (T-1) * B, state dim * num classes)\n",
        "        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim).detach()  # ((imagination horizon) * (T-1) * B, rnn hidden)\n",
        "\n",
        "        # reward, done_flagsは分布なので平均値をとる\n",
        "        # ((imagination horizon + 1), (T-1) * B)\n",
        "        with torch.no_grad():\n",
        "            imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n",
        "            target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1)\n",
        "            imagined_done_flags = discount_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).base_dist.probs.view(cfg.imagination_horizon, -1)\n",
        "            discount_arr = cfg.discount * torch.round(imagined_done_flags)\n",
        "\n",
        "        # lambda targetの計算\n",
        "        lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n",
        "\n",
        "        # actorの損失を計算\n",
        "        objective = imagined_action_log_probs * ((lambda_target - target_values).detach())\n",
        "        discount_arr = torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[1:]])\n",
        "        discount = torch.cumprod(discount_arr, 0)\n",
        "        actor_loss = -torch.sum(torch.mean(discount * (objective + cfg.actor_entropy_scale * imagined_action_entropys), dim=1))\n",
        "\n",
        "        actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n",
        "        actor_optimizer.step()\n",
        "\n",
        "        # criticの損失を計算\n",
        "        value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n",
        "        value_dist = td.Independent(td.Normal(value_mean, 1),  1)\n",
        "        critic_loss = -torch.mean(discount.detach() * value_dist.log_prob(lambda_target.detach()).unsqueeze(-1))\n",
        "\n",
        "        critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    if (iteration + 1) % cfg.slow_critic_update == 0:\n",
        "        target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    # エピソードが終了した時に再初期化\n",
        "    if done:\n",
        "        print(f\"episode: {total_episode} total_reward: {total_reward:.8f}\")\n",
        "        print(f\"num iter: {iteration} kl loss: {kl_loss.item():.8f} obs loss: {obs_loss.item():.8f} \"\n",
        "              f\"rewrd loss: {reward_loss.item():.8f} discount loss: {discount_loss.item():.8f} \"\n",
        "              f\"critic loss: {critic_loss.item():.8f} actor loss: {actor_loss.item():.8f}\"\n",
        "        )\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        total_episode += 1\n",
        "        policy.reset()\n",
        "\n",
        "        # 一定エピソードごとに評価\n",
        "        if total_episode % cfg.eval_freq == 0:\n",
        "            eval_reward = evaluation(eval_env, policy, iteration, cfg)\n",
        "            trained_models.save(\"./\")\n",
        "            if eval_reward > best_reward:\n",
        "                best_reward = eval_reward\n",
        "                os.makedirs(\"./best_models\", exist_ok=True)\n",
        "                trained_models.save(\"./best_models\")\n",
        "\n",
        "            eval_env.reset()\n",
        "            policy.reset()\n",
        "\n",
        "trained_models.save(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d",
      "metadata": {
        "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d"
      },
      "source": [
        "## 6. モデルの保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3",
      "metadata": {
        "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3"
      },
      "outputs": [],
      "source": [
        "# モデルの保存(Google Driveの場合）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trained_models.save(\"drive/MyDrive/Colab Notebooks/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6",
      "metadata": {
        "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6"
      },
      "source": [
        "## 7. 学習済みパラメータで評価  \n",
        "- こちらの評価に用いている環境は，Omnicampus上で評価する際に用いる環境と同じになっています．\n",
        "- 今回のコンペティションではPublic / Privateの分類はないため，基本的には以下の実装の評価を性能の目安としていただくと良いと思います．  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e",
      "metadata": {
        "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e"
      },
      "outputs": [],
      "source": [
        "# 環境の読み込み\n",
        "env = make_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 学習済みモデルの読み込み\n",
        "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    rssm,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    reward_model,\n",
        "    discount_model,\n",
        "    actor,\n",
        "    critic\n",
        ")\n",
        "\n",
        "trained_models.load(\"./\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a745c7-62dd-4431-99de-4da0f1489a16",
      "metadata": {
        "id": "41a745c7-62dd-4431-99de-4da0f1489a16"
      },
      "outputs": [],
      "source": [
        "# 結果を動画で観てみるための関数\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    plt.figure(figsize=(8, 8), dpi=50)\n",
        "    patch = plt.imshow(frames[0], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        plt.title(\"Step %d\" % (i))\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e365b2-bc62-4fe9-a9d9-a94e42d382f2",
      "metadata": {
        "id": "12e365b2-bc62-4fe9-a9d9-a94e42d382f2"
      },
      "source": [
        "**環境のシードを固定して評価を行います．シードを変更しないでください．**\n",
        "- 変更した場合，Omnicampus上での評価と結果が異なります．  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c8746-7371-4142-bd92-dde301c2f33c",
      "metadata": {
        "id": "206c8746-7371-4142-bd92-dde301c2f33c"
      },
      "outputs": [],
      "source": [
        "env = make_env(seed=1234, max_steps=None)\n",
        "\n",
        "policy = Agent(encoder, rssm, actor)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs]\n",
        "actions = []\n",
        "\n",
        "while not done:\n",
        "    action = policy(obs, eval=True)\n",
        "    action_int = np.argmax(action)  # 環境に渡すときはint型\n",
        "\n",
        "    obs, reward, done, _ = env.step(action_int)\n",
        "\n",
        "    total_reward += reward\n",
        "    frames.append(obs)\n",
        "    actions.append(action_int)\n",
        "\n",
        "print('Total Reward:', total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0",
      "metadata": {
        "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c",
      "metadata": {
        "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c"
      },
      "source": [
        "今回，評価を行う際のrepeat actionは1に設定しています．  \n",
        "そのため，repeat actionをそれ以外に設定している場合，repeat actionの分だけ繰り返した行動を提出する形にしています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d",
      "metadata": {
        "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d"
      },
      "outputs": [],
      "source": [
        "# repeat actionに対応した行動に変換する\n",
        "submission_actions = np.zeros(len(actions) * env._skip)\n",
        "for start_idx in range(env._skip):\n",
        "    submission_actions[start_idx::env._skip] = np.array(actions)\n",
        "\n",
        "np.save(\"drive/MyDrive/submission\", submission_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebb2707-74f6-43f5-a7cd-99419b3db93d",
      "metadata": {
        "id": "4ebb2707-74f6-43f5-a7cd-99419b3db93d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}