{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "229d1070-9828-4b02-8e37-bb8d01af7b75",
      "metadata": {
        "id": "229d1070-9828-4b02-8e37-bb8d01af7b75"
      },
      "source": [
        "# STORM実装\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b986f379-97f5-4449-b4c6-7cc385d1f474",
      "metadata": {
        "id": "b986f379-97f5-4449-b4c6-7cc385d1f474"
      },
      "source": [
        "## 1. 準備  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.26.2 gym[atari]==0.26.2 gym[accept-rom-license]==0.26.2 autorom ale-py\n",
        "# !pip install gym==0.26.2 'gym[atari]==0.26.2' 'gym[accept-rom-license]==0.26.2' 'gym[other]==0.26.2' autorom ale-py\n",
        "# !pip install einops"
      ],
      "metadata": {
        "id": "5b9qtVaIMAB-"
      },
      "id": "5b9qtVaIMAB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf",
      "metadata": {
        "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf"
      },
      "source": [
        "### ライブラリインポート  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
      "metadata": {
        "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
        "jupyter": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import ResizeObservation\n",
        "import torch\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Normal, Categorical, OneHotCategorical, OneHotCategoricalStraightThrough\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.autograd.profiler as prof\n",
        "\n",
        "from einops import rearrange, repeat, reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %tensorboard --logdir './logs'"
      ],
      "metadata": {
        "id": "2ESEx5jfFTwJ"
      },
      "id": "2ESEx5jfFTwJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPUの確認"
      ],
      "metadata": {
        "id": "tk5TF5MW1Jwg"
      },
      "id": "tk5TF5MW1Jwg"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "WQXLmjGY1L4u"
      },
      "id": "WQXLmjGY1L4u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wjkagwyF1Ubo"
      },
      "id": "wjkagwyF1Ubo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c7819663-fffc-44e5-842f-779564dd8227",
      "metadata": {
        "id": "c7819663-fffc-44e5-842f-779564dd8227"
      },
      "source": [
        "## 2. 環境の設定  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e",
      "metadata": {
        "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e"
      },
      "source": [
        "### Repeat Action    \n",
        "- モデルによって変更する可能性があると想定している部分は以下のとおりです．\n",
        "    - 画像のレンダリングサイズ(ResizeObervationクラスのshape)．\n",
        "    - 同じ行動を繰り返す数（RepeatActionクラスのskip）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "976179d0-2365-4440-bd7c-7e07ae218901",
      "metadata": {
        "id": "976179d0-2365-4440-bd7c-7e07ae218901"
      },
      "outputs": [],
      "source": [
        "class RepeatAction(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n",
        "    \"\"\"\n",
        "    def __init__(self, env, skip=4, max_steps=100_000):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.max_steps = max_steps if max_steps else float(\"inf\")  # イテレーションの制限\n",
        "        self.steps = 0  # イテレーション回数のカウント\n",
        "        self.height = env.observation_space.shape[0]\n",
        "        self.width = env.observation_space.shape[1]\n",
        "        self._skip = skip\n",
        "        self.lives_info = None\n",
        "\n",
        "    def reset(self):\n",
        "        obs, info = self.env.reset()\n",
        "        self.lives_info = info['lives']\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.steps >= self.max_steps:  # 100kに達したら何も返さないようにする\n",
        "            print(\"Reached max iterations.\")\n",
        "            return None\n",
        "\n",
        "        total_reward = 0.0\n",
        "        self.steps += 1\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, _, info = self.env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            if self.steps >= self.max_steps:  # 100kに達したら終端にする\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # ライフが減ったかチェックする\n",
        "        current_lives_info = info['lives']\n",
        "        if current_lives_info < self.lives_info:\n",
        "            info['life_loss'] = True\n",
        "            self.lives_info = current_lives_info\n",
        "        else:\n",
        "            info['life_loss'] = False\n",
        "\n",
        "        return obs, total_reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae",
      "metadata": {
        "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae"
      },
      "source": [
        "## 3. 補助機能の実装  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの保存"
      ],
      "metadata": {
        "id": "W9O16hLxp2kx"
      },
      "id": "W9O16hLxp2kx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439e4333-d460-4ef2-9aab-5c6f673f8261",
      "metadata": {
        "id": "439e4333-d460-4ef2-9aab-5c6f673f8261"
      },
      "outputs": [],
      "source": [
        "# モデルパラメータをGoogleDriveに保存・後で読み込みするためのヘルパークラス\n",
        "class TrainedModels:\n",
        "    def __init__(self, *models) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        models : nn.Module\n",
        "            保存するモデル．複数モデルを渡すことができます．\n",
        "\n",
        "        使用例: trained_models = TraindModels(encoder, rssm, value_model, action_model)\n",
        "        \"\"\"\n",
        "        assert np.all([nn.Module in model.__class__.__bases__ for model in models]), \"Arguments for TrainedModels need to be nn models.\"\n",
        "\n",
        "        self.models = models\n",
        "\n",
        "    def save(self, dir: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを保存します．\n",
        "        パラメータのファイル名は01.pt, 02.pt, ... のように連番になっています．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\")\n",
        "            )\n",
        "\n",
        "    def load(self, dir: str, device: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを読み込みます．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        device : str\n",
        "            モデルをどのデバイス(CPU or GPU)に載せるかの設定．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\"),\n",
        "                    map_location=device\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### set seed"
      ],
      "metadata": {
        "id": "M79CuiJdp9NL"
      },
      "id": "M79CuiJdp9NL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7",
      "metadata": {
        "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=20010105) -> None:\n",
        "    \"\"\"\n",
        "    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        シード値．\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions"
      ],
      "metadata": {
        "id": "Y4pUAwnsQIKd"
      },
      "id": "Y4pUAwnsQIKd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### symlog"
      ],
      "metadata": {
        "id": "V_K6D_x5QXmS"
      },
      "id": "V_K6D_x5QXmS"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def symlog(x):\n",
        "    return torch.sign(x) * torch.log(1 + torch.abs(x))"
      ],
      "metadata": {
        "id": "P3fzmnBeQPI0"
      },
      "id": "P3fzmnBeQPI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### symexp"
      ],
      "metadata": {
        "id": "oSlxV1q3QbEf"
      },
      "id": "oSlxV1q3QbEf"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def symexp(x):\n",
        "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)"
      ],
      "metadata": {
        "id": "Qsn9NJCcQekW"
      },
      "id": "Qsn9NJCcQekW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SymLogLoss"
      ],
      "metadata": {
        "id": "Wd4xRVqjQgqR"
      },
      "id": "Wd4xRVqjQgqR"
    },
    {
      "cell_type": "code",
      "source": [
        "class SymLogLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        target = symlog(target)\n",
        "        return 0.5*F.mse_loss(output, target)"
      ],
      "metadata": {
        "id": "EIKrGHgjQkWI"
      },
      "id": "EIKrGHgjQkWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SymLogTwoHotLoss"
      ],
      "metadata": {
        "id": "fkQjCa9YQvBG"
      },
      "id": "fkQjCa9YQvBG"
    },
    {
      "cell_type": "code",
      "source": [
        "class SymLogTwoHotLoss(nn.Module):\n",
        "    def __init__(self, num_classes, lower_bound, upper_bound):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lower_bound = lower_bound\n",
        "        self.upper_bound = upper_bound\n",
        "        self.bin_length = (upper_bound - lower_bound) / (num_classes-1)\n",
        "\n",
        "        # use register buffer so that bins move with .cuda() automatically\n",
        "        self.bins: torch.Tensor\n",
        "        self.register_buffer(\n",
        "            'bins', torch.linspace(-20, 20, num_classes), persistent=False)\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        target = symlog(target) # (B, L)\n",
        "        assert target.min() >= self.lower_bound and target.max() <= self.upper_bound\n",
        "\n",
        "        index = torch.bucketize(target, self.bins)\n",
        "        diff = target - self.bins[index-1]  # -1 to get the lower bound\n",
        "        weight = diff / self.bin_length\n",
        "        weight = torch.clamp(weight, 0, 1)\n",
        "        weight = weight.unsqueeze(-1) # (1, 1)\n",
        "\n",
        "        target_prob = (1-weight)*F.one_hot(index-1, self.num_classes) + weight*F.one_hot(index, self.num_classes)\n",
        "\n",
        "        loss = -target_prob * F.log_softmax(output, dim=-1)\n",
        "        loss = loss.sum(dim=-1)\n",
        "        return loss.mean()\n",
        "\n",
        "    def decode(self, output):\n",
        "        return symexp(F.softmax(output, dim=-1) @ self.bins)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     loss_func = SymLogTwoHotLoss(255, -20, 20)\n",
        "#     output = torch.randn(1, 1, 255).requires_grad_()\n",
        "#     target = torch.ones(1).reshape(1, 1).float() * 0.1\n",
        "#     print(target)\n",
        "#     loss = loss_func(output, target)\n",
        "#     print(loss)\n",
        "\n",
        "    # prob = torch.ones(1, 1, 255)*0.5/255\n",
        "    # prob[0, 0, 128] = 0.5\n",
        "    # logits = torch.log(prob)\n",
        "    # print(loss_func.decode(logits), loss_func.bins[128])"
      ],
      "metadata": {
        "id": "WEhpypVzQyUb"
      },
      "id": "WEhpypVzQyUb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Blocks"
      ],
      "metadata": {
        "id": "2C8cVcFeC9yI"
      },
      "id": "2C8cVcFeC9yI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get subsequent mask"
      ],
      "metadata": {
        "id": "UaNakMFszCua"
      },
      "id": "UaNakMFszCua"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subsequent_mask(seq):\n",
        "    ''' For masking out the subsequent info. '''\n",
        "    batch_size, batch_length = seq.shape[:2]\n",
        "    subsequent_mask = (1 - torch.triu(\n",
        "        torch.ones((1, batch_length, batch_length), device=seq.device), diagonal=1)).bool() # (1, L, L)\n",
        "    return subsequent_mask"
      ],
      "metadata": {
        "id": "E5QmRteRzGmx"
      },
      "id": "E5QmRteRzGmx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get subsequent mask with batch length"
      ],
      "metadata": {
        "id": "CVLZHwT-zOyW"
      },
      "id": "CVLZHwT-zOyW"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subsequent_mask_with_batch_length(batch_length, device):\n",
        "    ''' For masking out the subsequent info. '''\n",
        "    subsequent_mask = (1 - torch.triu(torch.ones((1, batch_length, batch_length), device=device), diagonal=1)).bool()\n",
        "    return subsequent_mask"
      ],
      "metadata": {
        "id": "JaG9h_dWzSq2"
      },
      "id": "JaG9h_dWzSq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get vector mask"
      ],
      "metadata": {
        "id": "f_fzJrueBGZf"
      },
      "id": "f_fzJrueBGZf"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_mask(batch_length, device):\n",
        "    mask = torch.ones((1, 1, batch_length), device=device).bool()\n",
        "    # mask = torch.ones((1, batch_length, 1), device=device).bool()\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Mya52ngZBOnw"
      },
      "id": "Mya52ngZBOnw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot Product Attention"
      ],
      "metadata": {
        "id": "YuNalqFckCJ3"
      },
      "id": "YuNalqFckCJ3"
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, float('-inf')) # -1e9が使われてた\n",
        "\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "gNVcxGKUkI13"
      },
      "id": "gNVcxGKUkI13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention"
      ],
      "metadata": {
        "id": "vm9ZeBgZjTgX"
      },
      "id": "vm9ZeBgZjTgX"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "        residual = q\n",
        "\n",
        "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
        "        # Separate different heads: b x lq x n x dv\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        # Transpose for attention dot product: b x n x lq x dv\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
        "\n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # Transpose to move the head dimension back: b x lq x n x dv\n",
        "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "\n",
        "        q = self.layer_norm(q) # (B, L, d_model=512)\n",
        "\n",
        "        return q, attn\n",
        "\n",
        "    def forward_with_kv_cache(self, q, k_cache, v_cache, idx):\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), q.size(1), q.size(1)\n",
        "        residual = q\n",
        "        q = self.w_qs(residual).view(sz_b, 1, n_head, d_k)\n",
        "        k = self.w_ks(residual).view(sz_b, 1, n_head, d_k)\n",
        "        v = self.w_vs(residual).view(sz_b, 1, n_head, d_v)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        k_cache[idx] = torch.cat([k_cache[idx], k], dim=2)\n",
        "        v_cache[idx] = torch.cat([v_cache[idx], v], dim=2)\n",
        "        q, attn = self.attention(q, k_cache[idx], v_cache[idx])\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, 1, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "        q = self.layer_norm(q)\n",
        "        return q"
      ],
      "metadata": {
        "id": "oVutRLI-jYIE"
      },
      "id": "oVutRLI-jYIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positionwise Feed Forward"
      ],
      "metadata": {
        "id": "lqKsOpESyRBs"
      },
      "id": "lqKsOpESyRBs"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise\n",
        "        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise\n",
        "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.w_2(F.relu(self.w_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "XH7hkUfGyZ8l"
      },
      "id": "XH7hkUfGyZ8l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Block KV Cache"
      ],
      "metadata": {
        "id": "pWE9zOsoixkM"
      },
      "id": "pWE9zOsoixkM"
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlockKVCache(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.slf_attn = MultiHeadAttention(num_heads, feat_dim, feat_dim//num_heads, feat_dim//num_heads, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(feat_dim, hidden_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, q, k, v, idx=None, slf_attn_mask=None, kv_cache=False):\n",
        "        if kv_cache:\n",
        "            output = self.slf_attn.forward_with_kv_cache(q, k, v, idx)\n",
        "            output = self.pos_ffn(output)\n",
        "            return output\n",
        "        output, attn = self.slf_attn(q, k, v, mask=slf_attn_mask)\n",
        "        output = self.pos_ffn(output)\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "KdS7V7pei7bq"
      },
      "id": "KdS7V7pei7bq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding 1D"
      ],
      "metadata": {
        "id": "kGsbTlZyDZzf"
      },
      "id": "kGsbTlZyDZzf"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_length: int,\n",
        "        embed_dim: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_length = max_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.pos_emb = nn.Embedding(self.max_length, embed_dim)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        pos_emb = self.pos_emb(torch.arange(self.max_length, device=feat.device))\n",
        "        pos_emb = repeat(pos_emb, \"L D -> B L D\", B=feat.shape[0])\n",
        "\n",
        "        feat = feat + pos_emb[:, :feat.shape[1], :]\n",
        "        return feat\n",
        "\n",
        "    def forward_with_position(self, feat, position):\n",
        "        assert feat.shape[1] == 1\n",
        "        pos_emb = self.pos_emb(torch.arange(self.max_length, device=feat.device))\n",
        "        pos_emb = repeat(pos_emb, \"L D -> B L D\", B=feat.shape[0])\n",
        "\n",
        "        feat = feat + pos_emb[:, position:position+1, :]\n",
        "        return feat"
      ],
      "metadata": {
        "id": "WQ8ZRhpgEKlo"
      },
      "id": "WQ8ZRhpgEKlo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Blocks"
      ],
      "metadata": {
        "id": "vf0QKLnaiXX7"
      },
      "id": "vf0QKLnaiXX7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer KV cache\n"
      ],
      "metadata": {
        "id": "KiE468W5CZeW"
      },
      "id": "KiE468W5CZeW"
    },
    {
      "cell_type": "code",
      "source": [
        "class StochasticTransformerKVCache(nn.Module):\n",
        "    def __init__(self, stoch_dim, action_dim, feat_dim, num_layers, num_heads, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.action_dim = action_dim\n",
        "        self.feat_dim = feat_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = feat_dim // num_heads\n",
        "\n",
        "        # mix image_embedding and action\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Linear(stoch_dim+action_dim, feat_dim, bias=False),\n",
        "            nn.LayerNorm(feat_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(feat_dim, feat_dim, bias=False),\n",
        "            nn.LayerNorm(feat_dim)\n",
        "        )\n",
        "        self.position_encoding = PositionalEncoding1D(max_length=max_length, embed_dim=feat_dim)\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            AttentionBlockKVCache(feat_dim=feat_dim, hidden_dim=feat_dim*2, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(feat_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, samples, action, mask):\n",
        "        '''\n",
        "        Normal forward pass\n",
        "        mask: (1, L, L)\n",
        "        '''\n",
        "        action = F.one_hot(action.long(), self.action_dim).float()\n",
        "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
        "        feats = self.position_encoding(feats)\n",
        "        feats = self.layer_norm(feats)\n",
        "\n",
        "        for layer in self.layer_stack:\n",
        "            feats, attn = layer(feats, feats, feats, slf_attn_mask=mask)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # def reset_kv_cache_list(self, batch_size, dtype):\n",
        "    #     '''\n",
        "    #     Reset self.kv_cache_list\n",
        "    #     '''\n",
        "    #     self.kv_cache_list = []\n",
        "    #     for layer in self.layer_stack:\n",
        "    #         self.kv_cache_list.append(torch.zeros(size=(batch_size, 0, self.feat_dim), dtype=dtype, device=\"cuda\"))\n",
        "\n",
        "    def reset_kv_cache_list(self, batch_size, dtype):\n",
        "        self.key_cache_list = []\n",
        "        self.value_cache_list = []\n",
        "        for layer in self.layer_stack:\n",
        "            self.key_cache_list.append(torch.zeros(size=(batch_size, self.num_heads, 0, self.head_dim), dtype=dtype, device=\"cuda\"))\n",
        "            self.value_cache_list.append(torch.zeros(size=(batch_size, self.num_heads, 0, self.head_dim), dtype=dtype, device=\"cuda\"))\n",
        "\n",
        "    def forward_with_kv_cache(self, samples, action):\n",
        "        '''\n",
        "        Forward pass with kv_cache, cache stored in self.kv_cache_list\n",
        "        samples: last_flattened_sample (B, 1, n_classes*stoch_dim)\n",
        "        '''\n",
        "        assert samples.shape[1] == 1\n",
        "\n",
        "        action = F.one_hot(action.long(), self.action_dim).float()\n",
        "        feats = self.stem(torch.cat([samples, action], dim=-1)) # mix image_embedding and action # (B, 1, feat_dim=512)\n",
        "        feats = self.position_encoding.forward_with_position(feats, position=self.key_cache_list[0].shape[1]) # (B, 1, feat_dim)\n",
        "        feats = self.layer_norm(feats) # (B, 1, feat_dim=512)\n",
        "\n",
        "        for idx, layer in enumerate(self.layer_stack):\n",
        "            feats = layer(feats, self.key_cache_list, self.value_cache_list, idx, kv_cache=True)\n",
        "\n",
        "        return feats"
      ],
      "metadata": {
        "id": "yjXxnYNDCgva"
      },
      "id": "yjXxnYNDCgva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "fpOoNL3qwDvO"
      },
      "id": "fpOoNL3qwDvO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### percentile"
      ],
      "metadata": {
        "id": "WCT2k8M4wGWK"
      },
      "id": "WCT2k8M4wGWK"
    },
    {
      "cell_type": "code",
      "source": [
        "def percentile(x, percentage):\n",
        "    flat_x = torch.flatten(x)\n",
        "    kth = int(percentage*len(flat_x))\n",
        "    per = torch.kthvalue(flat_x, kth).values\n",
        "    return per"
      ],
      "metadata": {
        "id": "yz9pwbbrwJvb"
      },
      "id": "yz9pwbbrwJvb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculate λ return"
      ],
      "metadata": {
        "id": "BvB_QQKvwZSV"
      },
      "id": "BvB_QQKvwZSV"
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_lambda_return(rewards, values, termination, gamma, lam, dtype=torch.float32):\n",
        "    # Invert termination to have 0 if the episode ended and 1 otherwise\n",
        "    inv_termination = (termination * -1) + 1\n",
        "\n",
        "    batch_size, batch_length = rewards.shape[:2]\n",
        "    # gae_step = torch.zeros((batch_size, ), dtype=dtype, device=\"cuda\")\n",
        "    gamma_return = torch.zeros((batch_size, batch_length+1), dtype=dtype, device=\"cuda\")\n",
        "    gamma_return[:, -1] = values[:, -1]\n",
        "    for t in reversed(range(batch_length)):  # with last bootstrap\n",
        "        gamma_return[:, t] = \\\n",
        "            rewards[:, t] + \\\n",
        "            gamma * inv_termination[:, t] * (1-lam) * values[:, t] + \\\n",
        "            gamma * inv_termination[:, t] * lam * gamma_return[:, t+1]\n",
        "    return gamma_return[:, :-1]"
      ],
      "metadata": {
        "id": "cvgavQEcwcH5"
      },
      "id": "cvgavQEcwcH5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor Critic Agent"
      ],
      "metadata": {
        "id": "HBdEre5uw3kd"
      },
      "id": "HBdEre5uw3kd"
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticAgent(nn.Module):\n",
        "    def __init__(self, feat_dim, num_layers, hidden_dim, action_dim, gamma, lambd, entropy_coef) -> None:\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.lambd = lambd\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.use_amp = True\n",
        "        self.tensor_dtype = torch.bfloat16 if self.use_amp else torch.float32\n",
        "\n",
        "        self.symlog_twohot_loss = SymLogTwoHotLoss(255, -20, 20)\n",
        "\n",
        "        actor = [\n",
        "            nn.Linear(feat_dim, hidden_dim, bias=False),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        for i in range(num_layers - 1):\n",
        "            actor.extend([\n",
        "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "        self.actor = nn.Sequential(\n",
        "            *actor,\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        critic = [\n",
        "            nn.Linear(feat_dim, hidden_dim, bias=False),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        for i in range(num_layers - 1):\n",
        "            critic.extend([\n",
        "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            *critic,\n",
        "            nn.Linear(hidden_dim, 255)\n",
        "        )\n",
        "        self.slow_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.lowerbound_ema = EMAScalar(decay=0.99)\n",
        "        self.upperbound_ema = EMAScalar(decay=0.99)\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=4e-5, eps=1e-5)\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_slow_critic(self, decay=0.98):\n",
        "        for slow_param, param in zip(self.slow_critic.parameters(), self.critic.parameters()):\n",
        "            slow_param.data.copy_(slow_param.data * decay + param.data * (1 - decay))\n",
        "\n",
        "    def policy(self, x):\n",
        "        logits = self.actor(x)\n",
        "        return logits\n",
        "\n",
        "    def value(self, x):\n",
        "        value = self.critic(x)\n",
        "        value = self.symlog_twohot_loss.decode(value)\n",
        "        return value\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def slow_value(self, x):\n",
        "        value = self.slow_critic(x)\n",
        "        value = self.symlog_twohot_loss.decode(value)\n",
        "        return value\n",
        "\n",
        "    def get_logits_raw_value(self, x):\n",
        "        logits = self.actor(x)\n",
        "        raw_value = self.critic(x)\n",
        "        return logits, raw_value\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, latent, greedy=False):\n",
        "        # latentはpriorとtransformerのhをconcatしたもの\n",
        "        self.eval()\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.use_amp):\n",
        "            logits = self.policy(latent)\n",
        "            dist = distributions.Categorical(logits=logits)\n",
        "            if greedy:\n",
        "                action = dist.probs.argmax(dim=-1)\n",
        "            else:\n",
        "                action = dist.sample()\n",
        "        return action\n",
        "\n",
        "    def sample_as_env_action(self, latent, greedy=False):\n",
        "        action = self.sample(latent, greedy)\n",
        "        return action.detach().cpu().squeeze(-1).item()\n",
        "\n",
        "    def update(self, latent, action, old_logprob, old_value, reward, termination, logger=None):\n",
        "        '''\n",
        "        Update policy and value model\n",
        "        '''\n",
        "        self.train()\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.use_amp):\n",
        "            logits, raw_value = self.get_logits_raw_value(latent) # (B, L+1, action_dim) # (B, L+1, 255)\n",
        "            dist = distributions.Categorical(logits=logits[:, :-1]) # (B, L, action_dim)\n",
        "            log_prob = dist.log_prob(action) # (B, L)\n",
        "            entropy = dist.entropy() # (B, L)\n",
        "\n",
        "            # decode value, calc lambda return\n",
        "            slow_value = self.slow_value(latent) # (B, L+1) # 勾配なし\n",
        "            slow_lambda_return = calc_lambda_return(reward, slow_value, termination, self.gamma, self.lambd) # (B, L)\n",
        "            value = self.symlog_twohot_loss.decode(raw_value) # (B, L+1)\n",
        "            lambda_return = calc_lambda_return(reward, value, termination, self.gamma, self.lambd) # (B, L)\n",
        "\n",
        "            # update value function with slow critic regularization\n",
        "            value_loss = self.symlog_twohot_loss(raw_value[:, :-1], lambda_return.detach()) #\n",
        "            slow_value_regularization_loss = self.symlog_twohot_loss(raw_value[:, :-1], slow_lambda_return.detach())\n",
        "\n",
        "            lower_bound = self.lowerbound_ema(percentile(lambda_return, 0.05))\n",
        "            upper_bound = self.upperbound_ema(percentile(lambda_return, 0.95))\n",
        "            S = upper_bound-lower_bound\n",
        "            norm_ratio = torch.max(torch.ones(1).cuda(), S)  # max(1, S) in the paper\n",
        "            norm_advantage = (lambda_return-value[:, :-1]) / norm_ratio\n",
        "            policy_loss = -(log_prob * norm_advantage.detach()).mean()\n",
        "\n",
        "            entropy_loss = entropy.mean()\n",
        "\n",
        "            loss = policy_loss + value_loss + slow_value_regularization_loss - self.entropy_coef * entropy_loss\n",
        "\n",
        "        # gradient descent\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.unscale_(self.optimizer)  # for clip grad\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=100.0)\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        self.update_slow_critic()\n",
        "\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.log('ActorCritic/policy_loss', policy_loss.item())\n",
        "            logger.log('ActorCritic/value_loss', value_loss.item())\n",
        "            logger.log('ActorCritic/entropy_loss', - entropy_loss.item())\n",
        "            logger.log('ActorCritic/S', S.item())\n",
        "            logger.log('ActorCritic/norm_ratio', norm_ratio.item())\n",
        "            logger.log('ActorCritic/slow_value_reg', slow_value_regularization_loss.item())\n",
        "            logger.log('ActorCritic/total_loss', loss.item())"
      ],
      "metadata": {
        "id": "JEfUpJE9w6h-"
      },
      "id": "JEfUpJE9w6h-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0662612e-701b-41a2-8679-25ad03fef367",
      "metadata": {
        "id": "0662612e-701b-41a2-8679-25ad03fef367"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## World Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "hXe_t1i7udVB"
      },
      "id": "hXe_t1i7udVB"
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBN(nn.Module):\n",
        "    def __init__(self, in_channels, stem_channels, final_feature_width) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        backbone = []\n",
        "        # stem\n",
        "        backbone.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=stem_channels,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                bias=False\n",
        "            )\n",
        "        )\n",
        "        feature_width = 64//2\n",
        "        channels = stem_channels\n",
        "        backbone.append(nn.BatchNorm2d(stem_channels))\n",
        "        backbone.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # layers\n",
        "        while True:\n",
        "            backbone.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=channels,\n",
        "                    out_channels=channels*2,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=False\n",
        "                )\n",
        "            )\n",
        "            channels *= 2\n",
        "            feature_width //= 2\n",
        "            backbone.append(nn.BatchNorm2d(channels))\n",
        "            backbone.append(nn.ReLU(inplace=True))\n",
        "\n",
        "            if feature_width == final_feature_width:\n",
        "                break\n",
        "\n",
        "        self.backbone = nn.Sequential(*backbone)\n",
        "        self.last_channels = channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = rearrange(x, \"B L C H W -> (B L) C H W\")\n",
        "        x = self.backbone(x)\n",
        "        # start_time = time.time()\n",
        "        # for i in range(len(self.backbone)):\n",
        "        #     print(self.backbone[i])\n",
        "        #     x = self.backbone[i](x)  # Conv2d\n",
        "        #     end_time = time.time()\n",
        "        #     print(f\"time: {end_time - start_time:.4f} s\")\n",
        "        #     start_time = time.time()\n",
        "        #     print(f'device:{x.device}, type:{x.dtype}')\n",
        "        # time.time()\n",
        "        # print('--------------------')\n",
        "        x = rearrange(x, \"(B L) C H W -> B L (C H W)\", B=batch_size)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hHtbvmUvuzUj"
      },
      "id": "hHtbvmUvuzUj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ewkSFqM13fhU"
      },
      "id": "ewkSFqM13fhU"
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBN(nn.Module):\n",
        "    def __init__(self, stoch_dim, last_channels, original_in_channels, stem_channels, final_feature_width) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        backbone = []\n",
        "        # stem\n",
        "        backbone.append(nn.Linear(stoch_dim, last_channels*final_feature_width*final_feature_width, bias=False))\n",
        "        backbone.append(Rearrange('B L (C H W) -> (B L) C H W', C=last_channels, H=final_feature_width))\n",
        "        backbone.append(nn.BatchNorm2d(last_channels))\n",
        "        backbone.append(nn.ReLU(inplace=True))\n",
        "        # residual_layer\n",
        "        # backbone.append(ResidualStack(last_channels, 1, last_channels//4))\n",
        "        # layers\n",
        "        channels = last_channels\n",
        "        feat_width = final_feature_width\n",
        "        while True:\n",
        "            if channels == stem_channels:\n",
        "                break\n",
        "            backbone.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=channels,\n",
        "                    out_channels=channels//2,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=False\n",
        "                )\n",
        "            )\n",
        "            channels //= 2\n",
        "            feat_width *= 2\n",
        "            backbone.append(nn.BatchNorm2d(channels))\n",
        "            backbone.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        backbone.append(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=channels,\n",
        "                out_channels=original_in_channels,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            )\n",
        "        )\n",
        "        self.backbone = nn.Sequential(*backbone)\n",
        "\n",
        "    def forward(self, sample):\n",
        "        batch_size = sample.shape[0]\n",
        "        obs_hat = self.backbone(sample)\n",
        "        obs_hat = rearrange(obs_hat, \"(B L) C H W -> B L C H W\", B=batch_size)\n",
        "        return obs_hat"
      ],
      "metadata": {
        "id": "iMAskLE-3iUB"
      },
      "id": "iMAskLE-3iUB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dist Head"
      ],
      "metadata": {
        "id": "Ff9vFDq7Ql3M"
      },
      "id": "Ff9vFDq7Ql3M"
    },
    {
      "cell_type": "code",
      "source": [
        "class DistHead(nn.Module):\n",
        "    '''\n",
        "    Dist: abbreviation of distribution\n",
        "    '''\n",
        "    def __init__(self, image_feat_dim, transformer_hidden_dim, stoch_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.stoch_dim = stoch_dim\n",
        "        self.post_head = nn.Linear(image_feat_dim, stoch_dim*stoch_dim)\n",
        "        self.prior_head = nn.Linear(transformer_hidden_dim, stoch_dim*stoch_dim)\n",
        "\n",
        "    def unimix(self, logits, mixing_ratio=0.01):\n",
        "        # uniform noise mixing\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        mixed_probs = mixing_ratio * torch.ones_like(probs) / self.stoch_dim + (1-mixing_ratio) * probs\n",
        "        logits = torch.log(mixed_probs)\n",
        "        return logits\n",
        "\n",
        "    def forward_post(self, x):\n",
        "        logits = self.post_head(x)\n",
        "        logits = rearrange(logits, \"B L (K C) -> B L K C\", K=self.stoch_dim)\n",
        "        logits = self.unimix(logits)\n",
        "        return logits\n",
        "\n",
        "    def forward_prior(self, x):\n",
        "        logits = self.prior_head(x)\n",
        "        logits = rearrange(logits, \"B L (K C) -> B L K C\", K=self.stoch_dim)\n",
        "        logits = self.unimix(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "DpipfEIQQpBQ"
      },
      "id": "DpipfEIQQpBQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Decoder"
      ],
      "metadata": {
        "id": "zTy4fntCUp4L"
      },
      "id": "zTy4fntCUp4L"
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardDecoder(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_size, transformer_hidden_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.head = nn.Linear(transformer_hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        feat = self.backbone(feat)\n",
        "        reward = self.head(feat)\n",
        "        return reward"
      ],
      "metadata": {
        "id": "Q4hvfbbhU13p"
      },
      "id": "Q4hvfbbhU13p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Termination Decoder"
      ],
      "metadata": {
        "id": "1BLiBDXLWYuK"
      },
      "id": "1BLiBDXLWYuK"
    },
    {
      "cell_type": "code",
      "source": [
        "class TerminationDecoder(nn.Module):\n",
        "    def __init__(self,  embedding_size, transformer_hidden_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(transformer_hidden_dim, transformer_hidden_dim, bias=False),\n",
        "            nn.LayerNorm(transformer_hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(transformer_hidden_dim, 1),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, feat):\n",
        "        feat = self.backbone(feat)\n",
        "        termination = self.head(feat)\n",
        "        termination = termination.squeeze(-1)  # remove last 1 dim\n",
        "        return termination"
      ],
      "metadata": {
        "id": "dEb1IP3dWdNv"
      },
      "id": "dEb1IP3dWdNv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSELoss"
      ],
      "metadata": {
        "id": "GqGMS6jYXSZX"
      },
      "id": "GqGMS6jYXSZX"
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, obs_hat, obs):\n",
        "        loss = (obs_hat - obs)**2\n",
        "        loss = reduce(loss, \"B L C H W -> B L\", \"sum\")\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "W5RzYMN5XVg4"
      },
      "id": "W5RzYMN5XVg4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CategoricalKLDivLossWithFreeBits\n"
      ],
      "metadata": {
        "id": "mxq2T6qQXpuF"
      },
      "id": "mxq2T6qQXpuF"
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalKLDivLossWithFreeBits(nn.Module):\n",
        "    def __init__(self, free_bits) -> None:\n",
        "        super().__init__()\n",
        "        self.free_bits = free_bits\n",
        "\n",
        "    def forward(self, p_logits, q_logits):\n",
        "        p_dist = OneHotCategorical(logits=p_logits)\n",
        "        q_dist = OneHotCategorical(logits=q_logits)\n",
        "        kl_div = torch.distributions.kl.kl_divergence(p_dist, q_dist)\n",
        "        kl_div = reduce(kl_div, \"B L D -> B L\", \"sum\")\n",
        "        kl_div = kl_div.mean()\n",
        "        real_kl_div = kl_div\n",
        "        kl_div = torch.max(torch.ones_like(kl_div)*self.free_bits, kl_div)\n",
        "        return kl_div, real_kl_div"
      ],
      "metadata": {
        "id": "96-WIrvJXwS_"
      },
      "id": "96-WIrvJXwS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### World Model"
      ],
      "metadata": {
        "id": "uQnU0ewhrkdE"
      },
      "id": "uQnU0ewhrkdE"
    },
    {
      "cell_type": "code",
      "source": [
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, in_channels, action_dim,\n",
        "                 transformer_max_length, transformer_hidden_dim, transformer_num_layers, transformer_num_heads):\n",
        "        super().__init__()\n",
        "        self.transformer_hidden_dim = transformer_hidden_dim\n",
        "        self.final_feature_width = 4\n",
        "        self.stoch_dim = 32\n",
        "        self.stoch_flattened_dim = self.stoch_dim*self.stoch_dim\n",
        "        self.use_amp = True\n",
        "        self.tensor_dtype = torch.bfloat16 if self.use_amp else torch.float32\n",
        "        self.imagine_batch_size = -1\n",
        "        self.imagine_batch_length = -1\n",
        "\n",
        "        self.encoder = EncoderBN(\n",
        "            in_channels=in_channels,\n",
        "            stem_channels=32,\n",
        "            final_feature_width=self.final_feature_width\n",
        "        )\n",
        "        self.storm_transformer = StochasticTransformerKVCache(\n",
        "            stoch_dim=self.stoch_flattened_dim,\n",
        "            action_dim=action_dim,\n",
        "            feat_dim=transformer_hidden_dim,\n",
        "            num_layers=transformer_num_layers,\n",
        "            num_heads=transformer_num_heads,\n",
        "            max_length=transformer_max_length,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.dist_head = DistHead(\n",
        "            image_feat_dim=self.encoder.last_channels*self.final_feature_width*self.final_feature_width,\n",
        "            transformer_hidden_dim=transformer_hidden_dim,\n",
        "            stoch_dim=self.stoch_dim\n",
        "        )\n",
        "        self.image_decoder = DecoderBN(\n",
        "            stoch_dim=self.stoch_flattened_dim,\n",
        "            last_channels=self.encoder.last_channels,\n",
        "            original_in_channels=in_channels,\n",
        "            stem_channels=32,\n",
        "            final_feature_width=self.final_feature_width\n",
        "        )\n",
        "        self.reward_decoder = RewardDecoder(\n",
        "            num_classes=255,\n",
        "            embedding_size=self.stoch_flattened_dim,\n",
        "            transformer_hidden_dim=transformer_hidden_dim\n",
        "        )\n",
        "        self.termination_decoder = TerminationDecoder(\n",
        "            embedding_size=self.stoch_flattened_dim,\n",
        "            transformer_hidden_dim=transformer_hidden_dim\n",
        "        )\n",
        "\n",
        "        self.mse_loss_func = MSELoss()\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.bce_with_logits_loss_func = nn.BCEWithLogitsLoss()\n",
        "        self.symlog_twohot_loss_func = SymLogTwoHotLoss(num_classes=255, lower_bound=-20, upper_bound=20)\n",
        "        self.categorical_kl_div_loss = CategoricalKLDivLossWithFreeBits(free_bits=1)\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
        "\n",
        "    def encode_obs(self, obs):\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.use_amp):\n",
        "            embedding = self.encoder(obs) # (B, L, image_feat_dim)\n",
        "            post_logits = self.dist_head.forward_post(embedding) # (B, L, n_classes, stoch_dim)\n",
        "            sample = self.stright_throught_gradient(post_logits, sample_mode=\"random_sample\") # (B, L, n_classes, stoch_dim) # one-hot\n",
        "            flattened_sample = self.flatten_sample(sample) # (B, L, n_classes*stoch_dim)\n",
        "        return flattened_sample\n",
        "\n",
        "    def calc_last_dist_feat(self, latent, action):\n",
        "        # 事前分布のサンプルと、Transformerからの最後の出力得る\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.use_amp):\n",
        "            temporal_mask = get_subsequent_mask(latent) # (1, L, L) # 下三角行列\n",
        "            dist_feat = self.storm_transformer(latent, action, temporal_mask) # (B, L, n_heads*d_v)\n",
        "            last_dist_feat = dist_feat[:, -1:] # (B, 1, n_heads*d_v)\n",
        "            prior_logits = self.dist_head.forward_prior(last_dist_feat) # (B, 1, n_classes, stoch_dim)\n",
        "            prior_sample = self.stright_throught_gradient(prior_logits, sample_mode=\"random_sample\") # (B, 1, n_classes, stoch_dim) # one-hot\n",
        "            prior_flattened_sample = self.flatten_sample(prior_sample) # (B, 1, n_classes*stoch_dim)\n",
        "        return prior_flattened_sample, last_dist_feat # (B, 1, n_classes*stoch_dim) # (B, 1, n_heads*d_v)\n",
        "\n",
        "    def predict_next(self, last_flattened_sample, action, log_video=True):\n",
        "        '''\n",
        "        last_flattened_sample: (B, 1, n_classes*stoch_dim)\n",
        "        action: (B, 1)\n",
        "        '''\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.use_amp):\n",
        "            dist_feat = self.storm_transformer.forward_with_kv_cache(last_flattened_sample, action) # (B, 1, 512) # Transformerからの出力h\n",
        "            prior_logits = self.dist_head.forward_prior(dist_feat) # (B, 1, n_classes, stoch_dim)\n",
        "\n",
        "            # decoding\n",
        "            prior_sample = self.stright_throught_gradient(prior_logits, sample_mode=\"random_sample\") # (B, 1, n_classes, stoch_dim) # one-hot\n",
        "            prior_flattened_sample = self.flatten_sample(prior_sample) # (B, 1, n_classes*stoch_dim)\n",
        "            if log_video:\n",
        "                obs_hat = self.image_decoder(prior_flattened_sample)\n",
        "            else:\n",
        "                obs_hat = None\n",
        "            reward_hat = self.reward_decoder(dist_feat) # (B, 1, num_classes=255)\n",
        "            reward_hat = self.symlog_twohot_loss_func.decode(reward_hat) # (1024, 1)\n",
        "            termination_hat = self.termination_decoder(dist_feat) # (1024, 1)\n",
        "            termination_hat = termination_hat > 0\n",
        "        return obs_hat, reward_hat, termination_hat, prior_flattened_sample, dist_feat\n",
        "\n",
        "    def stright_throught_gradient(self, logits, sample_mode=\"random_sample\"):\n",
        "        dist = OneHotCategorical(logits=logits)\n",
        "        if sample_mode == \"random_sample\":\n",
        "            sample = dist.sample() + dist.probs - dist.probs.detach()\n",
        "        elif sample_mode == \"mode\":\n",
        "            sample = dist.mode\n",
        "        elif sample_mode == \"probs\":\n",
        "            sample = dist.probs\n",
        "        return sample\n",
        "\n",
        "    def flatten_sample(self, sample):\n",
        "        return rearrange(sample, \"B L K C -> B L (K C)\")\n",
        "\n",
        "    def init_imagine_buffer(self, imagine_batch_size, imagine_batch_length, dtype):\n",
        "        '''\n",
        "        This can slightly improve the efficiency of imagine_data\n",
        "        But may vary across different machines\n",
        "        '''\n",
        "        if self.imagine_batch_size != imagine_batch_size or self.imagine_batch_length != imagine_batch_length:\n",
        "            print(f\"init_imagine_buffer: {imagine_batch_size}x{imagine_batch_length}@{dtype}\") # 1024 * 16\n",
        "            self.imagine_batch_size = imagine_batch_size\n",
        "            self.imagine_batch_length = imagine_batch_length\n",
        "            latent_size = (imagine_batch_size, imagine_batch_length+1, self.stoch_flattened_dim)\n",
        "            hidden_size = (imagine_batch_size, imagine_batch_length+1, self.transformer_hidden_dim)\n",
        "            scalar_size = (imagine_batch_size, imagine_batch_length)\n",
        "            self.latent_buffer = torch.zeros(latent_size, dtype=dtype, device=\"cuda\")\n",
        "            self.hidden_buffer = torch.zeros(hidden_size, dtype=dtype, device=\"cuda\")\n",
        "            self.action_buffer = torch.zeros(scalar_size, dtype=dtype, device=\"cuda\")\n",
        "            self.reward_hat_buffer = torch.zeros(scalar_size, dtype=dtype, device=\"cuda\")\n",
        "            self.termination_hat_buffer = torch.zeros(scalar_size, dtype=dtype, device=\"cuda\")\n",
        "\n",
        "    def imagine_data(self, agent: ActorCriticAgent, sample_obs, sample_action,\n",
        "                     imagine_batch_size, imagine_batch_length, log_video, logger):\n",
        "        self.init_imagine_buffer(imagine_batch_size, imagine_batch_length, dtype=self.tensor_dtype)\n",
        "        obs_hat_list = []\n",
        "\n",
        "        self.storm_transformer.reset_kv_cache_list(imagine_batch_size, dtype=self.tensor_dtype)\n",
        "        # context imagineを開始する前の準備\n",
        "        # print('imagine dataで呼び出し', sample_obs.shape)\n",
        "        # start = time.time()\n",
        "        context_latent = self.encode_obs(sample_obs) # (B, L, n_classes*stoch_dim)\n",
        "        # end = time.time()\n",
        "        # print(f\"encode_obs time: {end - start:.4f} s\")\n",
        "        # start = time.time()\n",
        "        for i in range(sample_obs.shape[1]):  # context_length is sample_obs.shape[1]\n",
        "            last_obs_hat, last_reward_hat, last_termination_hat, last_latent, last_dist_feat = self.predict_next(\n",
        "                context_latent[:, i:i+1],\n",
        "                sample_action[:, i:i+1],\n",
        "                log_video=log_video\n",
        "            )\n",
        "        # end = time.time()\n",
        "        # print('get context', end - start)\n",
        "        self.latent_buffer[:, 0:1] = last_latent # (B, 1, n_classes*stoch_dim) # 事前分布のsample(one-hot)をflattenしたもの\n",
        "        self.hidden_buffer[:, 0:1] = last_dist_feat # (B, 1, 512) # Transformerからの出力h\n",
        "\n",
        "        # imagine\n",
        "        # start = time.time()\n",
        "        for i in range(imagine_batch_length):\n",
        "            action = agent.sample(torch.cat([self.latent_buffer[:, i:i+1], self.hidden_buffer[:, i:i+1]], dim=-1))\n",
        "            self.action_buffer[:, i:i+1] = action # action: (B, 1)\n",
        "\n",
        "            last_obs_hat, last_reward_hat, last_termination_hat, last_latent, last_dist_feat = self.predict_next(\n",
        "                self.latent_buffer[:, i:i+1], self.action_buffer[:, i:i+1], log_video=log_video)\n",
        "\n",
        "            self.latent_buffer[:, i+1:i+2] = last_latent\n",
        "            self.hidden_buffer[:, i+1:i+2] = last_dist_feat\n",
        "            self.reward_hat_buffer[:, i:i+1] = last_reward_hat\n",
        "            self.termination_hat_buffer[:, i:i+1] = last_termination_hat\n",
        "            if log_video:\n",
        "                obs_hat_list.append(last_obs_hat[::imagine_batch_size//16])  # uniform sample vec_env\n",
        "        # end = time.time()\n",
        "        # print('imagine time', end - start)\n",
        "        if log_video:\n",
        "            logger.log(\"Imagine/predict_video\", torch.clamp(torch.cat(obs_hat_list, dim=1), 0, 1).cpu().float().detach().numpy())\n",
        "\n",
        "        return torch.cat([self.latent_buffer, self.hidden_buffer], dim=-1), self.action_buffer, self.reward_hat_buffer, self.termination_hat_buffer\n",
        "\n",
        "    def update(self, obs, action, reward, termination, logger=None):\n",
        "        self.train()\n",
        "        batch_size, batch_length = obs.shape[:2]\n",
        "\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=self.use_amp):\n",
        "            # encoding\n",
        "            # print('updateで呼び出し', obs.shape)\n",
        "            embedding = self.encoder(obs) # (B, L, image_feat_dim) # image_feat_dimはCNN通した後のCHWをflattenした次元\n",
        "            post_logits = self.dist_head.forward_post(embedding) # (B, L, n_classes, stoch_dim)\n",
        "            sample = self.stright_throught_gradient(post_logits, sample_mode=\"random_sample\") # (B, L, n_classes, stoch_dim)\n",
        "            flattened_sample = self.flatten_sample(sample) # (B, L, n_classes*stoch_dim) # 事後分布のflattenされたサンプル\n",
        "\n",
        "            # decoding image\n",
        "            obs_hat = self.image_decoder(flattened_sample) # (B, L, C, H, W)\n",
        "\n",
        "            # transformer\n",
        "            temporal_mask = get_subsequent_mask_with_batch_length(batch_length, flattened_sample.device) # (1, L, L)\n",
        "            dist_feat = self.storm_transformer(flattened_sample, action, temporal_mask) # (B, L, n*d_v)\n",
        "            prior_logits = self.dist_head.forward_prior(dist_feat) # (B, L, n_classes, stoch_dim)\n",
        "            # decoding reward and termination with dist_feat\n",
        "            reward_hat = self.reward_decoder(dist_feat) # (B, L, num_classes)\n",
        "            termination_hat = self.termination_decoder(dist_feat) # (B, L, 1)\n",
        "\n",
        "            # env loss\n",
        "            reconstruction_loss = self.mse_loss_func(obs_hat, obs)\n",
        "            reward_loss = self.symlog_twohot_loss_func(reward_hat, reward)\n",
        "            termination_loss = self.bce_with_logits_loss_func(termination_hat, termination)\n",
        "            # dyn-rep loss\n",
        "            dynamics_loss, dynamics_real_kl_div = self.categorical_kl_div_loss(post_logits[:, 1:].detach(), prior_logits[:, :-1])\n",
        "            representation_loss, representation_real_kl_div = self.categorical_kl_div_loss(post_logits[:, 1:], prior_logits[:, :-1].detach())\n",
        "\n",
        "            total_loss = reconstruction_loss + reward_loss + termination_loss + 0.5*dynamics_loss + 0.1*representation_loss\n",
        "\n",
        "        # gradient descent\n",
        "        self.scaler.scale(total_loss).backward()\n",
        "        self.scaler.unscale_(self.optimizer)  # for clip grad\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1000.0)\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.log(\"WorldModel/reconstruction_loss\", reconstruction_loss.item())\n",
        "            logger.log(\"WorldModel/reward_loss\", reward_loss.item())\n",
        "            logger.log(\"WorldModel/termination_loss\", termination_loss.item())\n",
        "            logger.log(\"WorldModel/dynamics_loss\", dynamics_loss.item())\n",
        "            logger.log(\"WorldModel/dynamics_real_kl_div\", dynamics_real_kl_div.item())\n",
        "            logger.log(\"WorldModel/representation_loss\", representation_loss.item())\n",
        "            logger.log(\"WorldModel/representation_real_kl_div\", representation_real_kl_div.item())\n",
        "            logger.log(\"WorldModel/total_loss\", total_loss.item())"
      ],
      "metadata": {
        "id": "YsmyQ1Ivrr_1"
      },
      "id": "YsmyQ1Ivrr_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## その他の機能"
      ],
      "metadata": {
        "id": "Pfr2XYoI0RiX"
      },
      "id": "Pfr2XYoI0RiX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReplayBuffer"
      ],
      "metadata": {
        "id": "7_2QAyqu0Wj6"
      },
      "id": "7_2QAyqu0Wj6"
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, obs_shape, num_envs, max_length=int(1E6), warmup_length=50000, store_on_gpu=False) -> None:\n",
        "        self.store_on_gpu = store_on_gpu\n",
        "        if store_on_gpu:\n",
        "            self.obs_buffer = torch.empty((max_length//num_envs, num_envs, *obs_shape), dtype=torch.uint8, device=\"cuda\", requires_grad=False)\n",
        "            self.action_buffer = torch.empty((max_length//num_envs, num_envs), dtype=torch.uint8, device=\"cuda\", requires_grad=False)\n",
        "            self.reward_buffer = torch.empty((max_length//num_envs, num_envs), dtype=torch.float32, device=\"cuda\", requires_grad=False)\n",
        "            self.termination_buffer = torch.empty((max_length//num_envs, num_envs), dtype=torch.float32, device=\"cuda\", requires_grad=False)\n",
        "        else:\n",
        "            self.obs_buffer = np.empty((max_length//num_envs, num_envs, *obs_shape), dtype=np.uint8)\n",
        "            self.action_buffer = np.empty((max_length//num_envs, num_envs), dtype=np.float32)\n",
        "            self.reward_buffer = np.empty((max_length//num_envs, num_envs), dtype=np.float32)\n",
        "            self.termination_buffer = np.empty((max_length//num_envs, num_envs), dtype=np.float32)\n",
        "\n",
        "        self.length = 0\n",
        "        self.num_envs = num_envs\n",
        "        self.last_pointer = -1\n",
        "        self.max_length = max_length\n",
        "        self.warmup_length = warmup_length\n",
        "        self.external_buffer_length = None\n",
        "\n",
        "    def load_trajectory(self, path):\n",
        "        buffer = pickle.load(open(path, \"rb\"))\n",
        "        if self.store_on_gpu:\n",
        "            self.external_buffer = {name: torch.from_numpy(buffer[name]).to(\"cuda\") for name in buffer}\n",
        "        else:\n",
        "            self.external_buffer = buffer\n",
        "        self.external_buffer_length = self.external_buffer[\"obs\"].shape[0]\n",
        "\n",
        "    def sample_external(self, batch_size, batch_length, to_device=\"cuda\"):\n",
        "        indexes = np.random.randint(0, self.external_buffer_length+1-batch_length, size=batch_size)\n",
        "        if self.store_on_gpu:\n",
        "            obs = torch.stack([self.external_buffer[\"obs\"][idx:idx+batch_length] for idx in indexes])\n",
        "            action = torch.stack([self.external_buffer[\"action\"][idx:idx+batch_length] for idx in indexes])\n",
        "            reward = torch.stack([self.external_buffer[\"reward\"][idx:idx+batch_length] for idx in indexes])\n",
        "            termination = torch.stack([self.external_buffer[\"done\"][idx:idx+batch_length] for idx in indexes])\n",
        "        else:\n",
        "            obs = np.stack([self.external_buffer[\"obs\"][idx:idx+batch_length] for idx in indexes])\n",
        "            action = np.stack([self.external_buffer[\"action\"][idx:idx+batch_length] for idx in indexes])\n",
        "            reward = np.stack([self.external_buffer[\"reward\"][idx:idx+batch_length] for idx in indexes])\n",
        "            termination = np.stack([self.external_buffer[\"done\"][idx:idx+batch_length] for idx in indexes])\n",
        "        return obs, action, reward, termination\n",
        "\n",
        "    def ready(self):\n",
        "        return self.length * self.num_envs > self.warmup_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size, external_batch_size, batch_length, to_device=\"cuda\"):\n",
        "        if self.store_on_gpu:\n",
        "            obs, action, reward, termination = [], [], [], []\n",
        "            if batch_size > 0:\n",
        "                for i in range(self.num_envs):\n",
        "                    indexes = np.random.randint(0, self.length+1-batch_length, size=batch_size//self.num_envs)\n",
        "                    obs.append(torch.stack([self.obs_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    action.append(torch.stack([self.action_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    reward.append(torch.stack([self.reward_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    termination.append(torch.stack([self.termination_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "\n",
        "            if self.external_buffer_length is not None and external_batch_size > 0:\n",
        "                external_obs, external_action, external_reward, external_termination = self.sample_external(\n",
        "                    external_batch_size, batch_length, to_device)\n",
        "                obs.append(external_obs)\n",
        "                action.append(external_action)\n",
        "                reward.append(external_reward)\n",
        "                termination.append(external_termination)\n",
        "\n",
        "            obs = torch.cat(obs, dim=0).float() / 255\n",
        "            obs = rearrange(obs, \"B T H W C -> B T C H W\") # Bのところは実際にはbatch_length * num_envsの長さかな？\n",
        "            action = torch.cat(action, dim=0)\n",
        "            reward = torch.cat(reward, dim=0)\n",
        "            termination = torch.cat(termination, dim=0)\n",
        "        else:\n",
        "            obs, action, reward, termination = [], [], [], []\n",
        "            if batch_size > 0:\n",
        "                for i in range(self.num_envs):\n",
        "                    indexes = np.random.randint(0, self.length+1-batch_length, size=batch_size//self.num_envs)\n",
        "                    obs.append(np.stack([self.obs_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    action.append(np.stack([self.action_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    reward.append(np.stack([self.reward_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "                    termination.append(np.stack([self.termination_buffer[idx:idx+batch_length, i] for idx in indexes]))\n",
        "\n",
        "            if self.external_buffer_length is not None and external_batch_size > 0:\n",
        "                external_obs, external_action, external_reward, external_termination = self.sample_external(\n",
        "                    external_batch_size, batch_length, to_device)\n",
        "                obs.append(external_obs)\n",
        "                action.append(external_action)\n",
        "                reward.append(external_reward)\n",
        "                termination.append(external_termination)\n",
        "\n",
        "            obs = torch.from_numpy(np.concatenate(obs, axis=0)).float().cuda() / 255\n",
        "            obs = rearrange(obs, \"B T H W C -> B T C H W\")\n",
        "            action = torch.from_numpy(np.concatenate(action, axis=0)).cuda()\n",
        "            reward = torch.from_numpy(np.concatenate(reward, axis=0)).cuda()\n",
        "            termination = torch.from_numpy(np.concatenate(termination, axis=0)).cuda()\n",
        "\n",
        "        return obs, action, reward, termination\n",
        "\n",
        "    def append(self, obs, action, reward, termination):\n",
        "        # obs/nex_obs: torch Tensor\n",
        "        # action/reward/termination: int or float or bool\n",
        "        self.last_pointer = (self.last_pointer + 1) % (self.max_length//self.num_envs)\n",
        "        if self.store_on_gpu:\n",
        "            self.obs_buffer[self.last_pointer] = torch.from_numpy(obs)\n",
        "            self.action_buffer[self.last_pointer] = torch.tensor(action, dtype=torch.uint8)\n",
        "            self.reward_buffer[self.last_pointer] = torch.tensor(reward, dtype=torch.float32)\n",
        "            self.termination_buffer[self.last_pointer] = torch.from_numpy(np.array(termination))\n",
        "        else:\n",
        "            self.obs_buffer[self.last_pointer] = obs\n",
        "            self.action_buffer[self.last_pointer] = action\n",
        "            self.reward_buffer[self.last_pointer] = reward\n",
        "            self.termination_buffer[self.last_pointer] = termination\n",
        "\n",
        "        if len(self) < self.max_length:\n",
        "            self.length += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length * self.num_envs"
      ],
      "metadata": {
        "id": "dY0Ap8My0WEg"
      },
      "id": "dY0Ap8My0WEg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "hAHg6gee2lFs"
      },
      "id": "hAHg6gee2lFs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EMA Scalar"
      ],
      "metadata": {
        "id": "xoqJKDoK2o-C"
      },
      "id": "xoqJKDoK2o-C"
    },
    {
      "cell_type": "code",
      "source": [
        "class EMAScalar():\n",
        "    def __init__(self, decay) -> None:\n",
        "        self.scalar = 0.0\n",
        "        self.decay = decay\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.get()\n",
        "\n",
        "    def update(self, value):\n",
        "        self.scalar = self.scalar * self.decay + value * (1 - self.decay)\n",
        "\n",
        "    def get(self):\n",
        "        return self.scalar"
      ],
      "metadata": {
        "id": "LoMOv98p2reN"
      },
      "id": "LoMOv98p2reN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logger"
      ],
      "metadata": {
        "id": "8z9QXtbhueHt"
      },
      "id": "8z9QXtbhueHt"
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger():\n",
        "    def __init__(self, path) -> None:\n",
        "        self.writer = SummaryWriter(path, flush_secs=1)\n",
        "        self.tag_step = {}\n",
        "\n",
        "    def log(self, tag, value):\n",
        "        if tag not in self.tag_step:\n",
        "            self.tag_step[tag] = 0\n",
        "        else:\n",
        "            self.tag_step[tag] += 1\n",
        "        if \"video\" in tag:\n",
        "            self.writer.add_video(tag, value, self.tag_step[tag], fps=15)\n",
        "        elif \"images\" in tag:\n",
        "            self.writer.add_images(tag, value, self.tag_step[tag])\n",
        "        elif \"hist\" in tag:\n",
        "            self.writer.add_histogram(tag, value, self.tag_step[tag])\n",
        "        else:\n",
        "            self.writer.add_scalar(tag, value, self.tag_step[tag])"
      ],
      "metadata": {
        "id": "2dH1CeESugAT"
      },
      "id": "2dH1CeESugAT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b06c188f-8a87-42e7-9f61-7f385eccc565",
      "metadata": {
        "id": "b06c188f-8a87-42e7-9f61-7f385eccc565"
      },
      "source": [
        "## 5. 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "DNhFa6PakHO8"
      },
      "id": "DNhFa6PakHO8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a14cd1-dc80-4f22-a199-131fa267e14c",
      "metadata": {
        "id": "48a14cd1-dc80-4f22-a199-131fa267e14c"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # basic settings\n",
        "        self.eval_freq = 1000\n",
        "        self.seed = 1234\n",
        "        self.ImageSize = 64\n",
        "        self.ReplayBufferOnGPU = True\n",
        "\n",
        "        # joint train agent\n",
        "        self.SampleMaxSteps = 100_000\n",
        "        self.BufferMaxLength = 100000\n",
        "        self.BufferWarmUp = 1024\n",
        "        self.NumEnvs = 1\n",
        "        self.BatchSize = 16\n",
        "        self.DemonstrationBatchSize = 4\n",
        "        self.BatchLength = 64\n",
        "        self.ImagineBatchSize = 1024\n",
        "        self.ImagineDemonstrationBatchSize = 256\n",
        "        self.ImagineContextLength = 8\n",
        "        self.ImagineBatchLength = 16\n",
        "        self.TrainDynamicsEverySteps = 1\n",
        "        self.TrainAgentEverySteps = 1\n",
        "        self.UseDemonstration = False\n",
        "        self.SaveEverySteps = 2500\n",
        "\n",
        "        # world models\n",
        "        self.WorldModel_InChannels = 3\n",
        "        self.WorldModel_TransformerMaxLength = 64\n",
        "        self.WorldModel_TransformerHiddenDim = 512\n",
        "        self.WorldModel_TransformerNumLayers = 2\n",
        "        self.WorldModel_TransformerNumHeads = 8\n",
        "\n",
        "        # Agent\n",
        "        self.Agent_NumLayers = 2\n",
        "        self.Agent_HiddenDim =  512\n",
        "        self.Agent_Gamma = 0.985\n",
        "        self.Agent_Lambda = 0.95\n",
        "        self.Agent_EntropyCoef = 3E-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make env"
      ],
      "metadata": {
        "id": "DSpsoYnSsuSE"
      },
      "id": "DSpsoYnSsuSE"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(seed=None, img_size=64, max_steps=100_000):\n",
        "    env = gym.make(\"ALE/MsPacman-v5\")\n",
        "\n",
        "    # シード固定\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    env = ResizeObservation(env, (img_size, img_size))\n",
        "    env = RepeatAction(env=env, skip=4, max_steps=max_steps)\n",
        "\n",
        "    return env"
      ],
      "metadata": {
        "id": "8aKSET75swOX"
      },
      "id": "8aKSET75swOX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build vec env"
      ],
      "metadata": {
        "id": "KqaPl02bml0C"
      },
      "id": "KqaPl02bml0C"
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_vec_env(env_name, image_size, num_envs, seed):\n",
        "#     # lambda pitfall refs to: https://python.plainenglish.io/python-pitfalls-with-variable-capture-dcfc113f39b7\n",
        "#     def lambda_generator(env_name, image_size):\n",
        "#         return lambda: make_env(seed=seed)\n",
        "#     env_fns = []\n",
        "#     env_fns = [lambda_generator(env_name, image_size) for i in range(num_envs)]\n",
        "#     vec_env = gym.vector.AsyncVectorEnv(env_fns=env_fns)\n",
        "#     return vec_env"
      ],
      "metadata": {
        "id": "Hsu8hh73sbxB"
      },
      "id": "Hsu8hh73sbxB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train World Models Step"
      ],
      "metadata": {
        "id": "gA76wQz6uIus"
      },
      "id": "gA76wQz6uIus"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_world_model_step(replay_buffer: ReplayBuffer, world_model: WorldModel, batch_size, demonstration_batch_size, batch_length, logger):\n",
        "    obs, action, reward, termination = replay_buffer.sample(batch_size, demonstration_batch_size, batch_length)\n",
        "    world_model.update(obs, action, reward, termination, logger=logger)"
      ],
      "metadata": {
        "id": "eiaEWTcMuOMI"
      },
      "id": "eiaEWTcMuOMI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### World Model Imagine Data"
      ],
      "metadata": {
        "id": "h4qgJf40gtzw"
      },
      "id": "h4qgJf40gtzw"
    },
    {
      "cell_type": "code",
      "source": [
        "def world_model_imagine_data(replay_buffer: ReplayBuffer,\n",
        "                             world_model: WorldModel, agent: ActorCriticAgent,\n",
        "                             imagine_batch_size, imagine_demonstration_batch_size,\n",
        "                             imagine_context_length, imagine_batch_length,\n",
        "                             log_video, logger):\n",
        "    '''\n",
        "    Sample context from replay buffer, then imagine data with world model and agent\n",
        "    '''\n",
        "    world_model.eval()\n",
        "    agent.eval()\n",
        "\n",
        "    sample_obs, sample_action, sample_reward, sample_termination = replay_buffer.sample(\n",
        "        imagine_batch_size, imagine_demonstration_batch_size, imagine_context_length)\n",
        "    # sample_obs: (B=1024, context=8, C, W, H)\n",
        "\n",
        "    latent, action, reward_hat, termination_hat = world_model.imagine_data(\n",
        "        agent, sample_obs, sample_action,\n",
        "        imagine_batch_size=imagine_batch_size+imagine_demonstration_batch_size,\n",
        "        imagine_batch_length=imagine_batch_length,\n",
        "        log_video=log_video,\n",
        "        logger=logger\n",
        "    )\n",
        "    # latent shape torch.Size([1024, 17, 1536]) 1536は32*32+transformer hidden_dim 17はimagine_length+1\n",
        "    # action shape torch.Size([1024, 16])\n",
        "    # reward_hat shape torch.Size([1024, 16])\n",
        "    # termination_hat shape torch.Size([1024, 16])\n",
        "    return latent, action, None, None, reward_hat, termination_hat"
      ],
      "metadata": {
        "id": "WB3rsvzSgwNt"
      },
      "id": "WB3rsvzSgwNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joint Train World Model Agent"
      ],
      "metadata": {
        "id": "QEboY8pIW9B5"
      },
      "id": "QEboY8pIW9B5"
    },
    {
      "cell_type": "code",
      "source": [
        "def joint_train_world_model_agent(env_name, max_steps, num_envs, image_size,\n",
        "                                  replay_buffer: ReplayBuffer,\n",
        "                                  world_model: WorldModel, agent: ActorCriticAgent,\n",
        "                                  train_dynamics_every_steps, train_agent_every_steps,\n",
        "                                  batch_size, demonstration_batch_size, batch_length,\n",
        "                                  imagine_batch_size, imagine_demonstration_batch_size,\n",
        "                                  imagine_context_length, imagine_batch_length,\n",
        "                                  save_every_steps, seed, logger, eval_freq):\n",
        "    # create ckpt dir\n",
        "    # os.makedirs(f\"ckpt\", exist_ok=True)\n",
        "\n",
        "    # build vec env, not useful in the Atari100k setting\n",
        "    # but when the max_steps is large, you can use parallel envs to speed up\n",
        "    # vec_env = build_vec_env(env_name, image_size, num_envs=num_envs, seed=seed)\n",
        "    env = make_env(seed=seed, max_steps=100_000)\n",
        "    # print(\"Current env: \" + colorama.Fore.YELLOW + f\"{env_name}\" + colorama.Style.RESET_ALL)\n",
        "\n",
        "    # reset envs and variables\n",
        "    sum_reward = 0\n",
        "    current_obs, current_info = env.reset() # (H, W, C)\n",
        "    context_obs = deque(maxlen=16)\n",
        "    context_action = deque(maxlen=16)\n",
        "\n",
        "    # sample and train\n",
        "    for total_steps in tqdm(range(max_steps//num_envs)):\n",
        "        # start = time.time()\n",
        "        # sample part >>>\n",
        "        if replay_buffer.ready():\n",
        "            world_model.eval()\n",
        "            agent.eval()\n",
        "            with torch.no_grad():\n",
        "                if len(context_action) == 0:\n",
        "                    action = env.action_space.sample() # (action_dim=1, )\n",
        "                else:\n",
        "                    # print('context_latentで呼び出し', torch.cat(list(context_obs), dim=1).shape)\n",
        "                    context_latent = world_model.encode_obs(torch.cat(list(context_obs), dim=1)) # (B=1?, L, n_classes*stoch_dim)\n",
        "                    model_context_action = np.stack(list(context_action), axis=1) # (B, L)\n",
        "                    model_context_action = torch.Tensor(model_context_action).cuda()\n",
        "                    prior_flattened_sample, last_dist_feat = world_model.calc_last_dist_feat(context_latent, model_context_action)\n",
        "                    action = agent.sample_as_env_action(\n",
        "                        torch.cat([prior_flattened_sample, last_dist_feat], dim=-1),\n",
        "                        greedy=False\n",
        "                    )\n",
        "            context_obs.append(rearrange(torch.Tensor(current_obs).cuda(), \"H W C -> 1 1 C H W\")/255)\n",
        "            context_action.append([action]) # []でB次元追加してる\n",
        "        else:\n",
        "            action = env.action_space.sample() # (action_dim=1, )\n",
        "\n",
        "        obs, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "        replay_buffer.append(current_obs, action, reward, np.logical_or(done, info[\"life_loss\"]))\n",
        "\n",
        "        if done:\n",
        "            logger.log(f\"sample/{env_name}_reward\", sum_reward)\n",
        "            logger.log(f\"sample/{env_name}_episode_steps\", current_info[\"episode_frame_number\"]//4)  # framskip=4\n",
        "            logger.log(\"replay_buffer/length\", len(replay_buffer))\n",
        "            sum_reward = 0\n",
        "            obs, info = env.reset()\n",
        "            done = False\n",
        "\n",
        "        # update current_obs, current_info and sum_reward\n",
        "        sum_reward += reward\n",
        "        current_obs = obs\n",
        "        current_info = info\n",
        "        # end = time.time()\n",
        "        # print('sample part time', end-start)\n",
        "        # <<< sample part\n",
        "\n",
        "        # train world model part >>>\n",
        "        # start = time.time()\n",
        "        if replay_buffer.ready() and total_steps % (train_dynamics_every_steps//num_envs) == 0:\n",
        "            train_world_model_step(\n",
        "                replay_buffer=replay_buffer,\n",
        "                world_model=world_model,\n",
        "                batch_size=batch_size,\n",
        "                demonstration_batch_size=demonstration_batch_size,\n",
        "                batch_length=batch_length,\n",
        "                logger=logger\n",
        "            )\n",
        "        # end = time.time()\n",
        "        # print('train world model time', end-start)\n",
        "        # <<< train world model part\n",
        "\n",
        "        # train agent part >>>\n",
        "        if replay_buffer.ready() and total_steps % (train_agent_every_steps//num_envs) == 0 and total_steps*num_envs >= 0:\n",
        "            if total_steps % (save_every_steps//num_envs) == 0:\n",
        "                log_video = False # True\n",
        "            else:\n",
        "                log_video = False\n",
        "\n",
        "            # start = time.time()\n",
        "            with torch.no_grad():\n",
        "                imagine_latent, agent_action, agent_logprob, agent_value, imagine_reward, imagine_termination = world_model_imagine_data(\n",
        "                    replay_buffer=replay_buffer,\n",
        "                    world_model=world_model,\n",
        "                    agent=agent,\n",
        "                    imagine_batch_size=imagine_batch_size,\n",
        "                    imagine_demonstration_batch_size=imagine_demonstration_batch_size,\n",
        "                    imagine_context_length=imagine_context_length,\n",
        "                    imagine_batch_length=imagine_batch_length,\n",
        "                    log_video=log_video,\n",
        "                    logger=logger\n",
        "                )\n",
        "            # end = time.time()\n",
        "            # print('imagine data time', end - start)\n",
        "            # imagine_latent shape torch.Size([1024, 17, 1536]) 1536は32*32+transformer hidden_dim 17はimagine_length+1\n",
        "            # agent_action shape torch.Size([1024, 16])\n",
        "            # None\n",
        "            # None\n",
        "            # imagine_reward shape torch.Size([1024, 16])\n",
        "            # imagine_termination shape torch.Size([1024, 16])\n",
        "\n",
        "            # start = time.time()\n",
        "            agent.update(\n",
        "                latent=imagine_latent,\n",
        "                action=agent_action,\n",
        "                old_logprob=agent_logprob,\n",
        "                old_value=agent_value,\n",
        "                reward=imagine_reward,\n",
        "                termination=imagine_termination,\n",
        "                logger=logger\n",
        "            )\n",
        "            # end = time.time()\n",
        "            # print('agent update time', end - start)\n",
        "        # <<< train agent part\n",
        "\n",
        "        # >>> eval part\n",
        "        if replay_buffer.ready() and (total_steps+1) % eval_freq == 0:\n",
        "            evaluate(world_model, agent, logger)\n",
        "        # <<< eval part\n",
        "\n",
        "        # save model per episode\n",
        "        # if total_steps % (save_every_steps//num_envs) == 0:\n",
        "        #     print(f\"Saving model at total steps {total_steps}\")\n",
        "        #     torch.save(world_model.state_dict(), f\"ckpt/world_model_{total_steps}.pth\")\n",
        "        #     torch.save(agent.state_dict(), f\"ckpt/agent_{total_steps}.pth\")\n"
      ],
      "metadata": {
        "id": "STQmEFtfXA3X"
      },
      "id": "STQmEFtfXA3X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build World Model"
      ],
      "metadata": {
        "id": "rN_nxN0-3kvh"
      },
      "id": "rN_nxN0-3kvh"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_world_model(config, action_dim):\n",
        "    return WorldModel(\n",
        "        in_channels=config.WorldModel_InChannels,\n",
        "        action_dim=action_dim,\n",
        "        transformer_max_length=config.WorldModel_TransformerMaxLength,\n",
        "        transformer_hidden_dim=config.WorldModel_TransformerHiddenDim,\n",
        "        transformer_num_layers=config.WorldModel_TransformerNumLayers,\n",
        "        transformer_num_heads=config.WorldModel_TransformerNumHeads\n",
        "    ).cuda()"
      ],
      "metadata": {
        "id": "EXLb82wr3n6T"
      },
      "id": "EXLb82wr3n6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Agent"
      ],
      "metadata": {
        "id": "oQ6ILwbITU_G"
      },
      "id": "oQ6ILwbITU_G"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(conf, action_dim):\n",
        "    return ActorCriticAgent(\n",
        "        feat_dim=32*32+config.WorldModel_TransformerHiddenDim,\n",
        "        num_layers=config.Agent_NumLayers,\n",
        "        hidden_dim=config.Agent_HiddenDim,\n",
        "        action_dim=action_dim,\n",
        "        gamma=config.Agent_Gamma,\n",
        "        lambd=config.Agent_Lambda,\n",
        "        entropy_coef=config.Agent_EntropyCoef,\n",
        "    ).cuda()"
      ],
      "metadata": {
        "id": "bm55cmpGUPpU"
      },
      "id": "bm55cmpGUPpU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate"
      ],
      "metadata": {
        "id": "1adA5M9_BgPF"
      },
      "id": "1adA5M9_BgPF"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(world_model, agent, logger):\n",
        "    env = make_env(seed=1234, img_size=64, max_steps=None)\n",
        "\n",
        "    world_model.eval()\n",
        "    agent.eval()\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    frames = []\n",
        "    actions = []\n",
        "    context_obs = deque(maxlen=16)\n",
        "    context_action = deque(maxlen=16)\n",
        "\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            if len(context_action) == 0:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                context_latent = world_model.encode_obs(torch.cat(list(context_obs), dim=1))\n",
        "                model_context_action = np.stack(list(context_action), axis=1)\n",
        "                model_context_action = torch.Tensor(context_action).cuda()\n",
        "                model_context_action = model_context_action.view(1, -1) # チェック\n",
        "                prior_flattened_sample, last_dist_feat = world_model.calc_last_dist_feat(context_latent, model_context_action)\n",
        "                action = agent.sample_as_env_action(\n",
        "                    torch.cat([prior_flattened_sample, last_dist_feat], dim=-1),\n",
        "                    greedy=True\n",
        "                )\n",
        "\n",
        "        context_obs.append(rearrange(torch.Tensor(obs).cuda(), \"H W C -> 1 1 C H W\")/255)\n",
        "        context_action.append([action])\n",
        "\n",
        "        obs, reward, done, info = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        frames.append(obs)\n",
        "        actions.append(action)\n",
        "\n",
        "    print('Total Reward:', total_reward)\n",
        "    logger.log(f\"evaluation/reward\", total_reward)\n",
        "    world_model.train()\n",
        "    agent.train()"
      ],
      "metadata": {
        "id": "tLgT-98cBUIZ"
      },
      "id": "tLgT-98cBUIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実際の学習"
      ],
      "metadata": {
        "id": "j2ubSwVsvA8O"
      },
      "id": "j2ubSwVsvA8O"
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# set seed\n",
        "set_seed()\n",
        "# tensorboard writer\n",
        "logger = Logger(path=\"./logs\")\n",
        "%tensorboard --logdir './logs'\n",
        "\n",
        "# getting action_dim with dummy env\n",
        "dummy_env = make_env(seed=0)\n",
        "action_dim = dummy_env.action_space.n\n",
        "\n",
        "# build world model and agent\n",
        "world_model = build_world_model(config, action_dim)\n",
        "agent = build_agent(config, action_dim)\n",
        "\n",
        "# build replay buffer\n",
        "replay_buffer = ReplayBuffer(\n",
        "    obs_shape=(config.ImageSize, config.ImageSize, 3),\n",
        "    num_envs=config.NumEnvs,\n",
        "    max_length=config.BufferMaxLength,\n",
        "    warmup_length=config.BufferWarmUp,\n",
        "    store_on_gpu=config.ReplayBufferOnGPU\n",
        ")\n",
        "\n",
        "# # judge whether to load demonstration trajectory\n",
        "# if config.UseDemonstration:\n",
        "#     print(colorama.Fore.MAGENTA + f\"loading demonstration trajectory from {args.trajectory_path}\" + colorama.Style.RESET_ALL)\n",
        "#     replay_buffer.load_trajectory(path=args.trajectory_path)\n",
        "\n",
        "# train\n",
        "joint_train_world_model_agent(\n",
        "    env_name='PacMan',\n",
        "    num_envs=config.NumEnvs,\n",
        "    max_steps=config.SampleMaxSteps,\n",
        "    image_size=config.ImageSize,\n",
        "    replay_buffer=replay_buffer,\n",
        "    world_model=world_model,\n",
        "    agent=agent,\n",
        "    train_dynamics_every_steps=config.TrainDynamicsEverySteps,\n",
        "    train_agent_every_steps=config.TrainAgentEverySteps,\n",
        "    batch_size=config.BatchSize,\n",
        "    demonstration_batch_size=config.DemonstrationBatchSize if config.UseDemonstration else 0,\n",
        "    batch_length=config.BatchLength,\n",
        "    imagine_batch_size=config.ImagineBatchSize,\n",
        "    imagine_demonstration_batch_size=config.ImagineDemonstrationBatchSize if config.UseDemonstration else 0,\n",
        "    imagine_context_length=config.ImagineContextLength,\n",
        "    imagine_batch_length=config.ImagineBatchLength,\n",
        "    save_every_steps=config.SaveEverySteps,\n",
        "    seed=config.seed,\n",
        "    logger=logger,\n",
        "    eval_freq=config.eval_freq\n",
        ")\n"
      ],
      "metadata": {
        "id": "iF__tf3VvLg5"
      },
      "id": "iF__tf3VvLg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d",
      "metadata": {
        "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d"
      },
      "source": [
        "## 6. モデルの保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3",
      "metadata": {
        "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3"
      },
      "outputs": [],
      "source": [
        "# # モデルの保存(Google Driveの場合）\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# trained_models.save(\"drive/MyDrive/Colab Notebooks/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存\n",
        "os.makedirs(\"./SavedModels\", exist_ok=True)\n",
        "trained_models = TrainedModels(\n",
        "    world_model,\n",
        "    agent\n",
        ")\n",
        "trained_models.save(\"./SavedModels\")"
      ],
      "metadata": {
        "id": "Bv7CZrWZlV93"
      },
      "id": "Bv7CZrWZlV93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6",
      "metadata": {
        "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6"
      },
      "source": [
        "## 7. 学習済みパラメータで評価  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e",
      "metadata": {
        "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e"
      },
      "outputs": [],
      "source": [
        "# 環境の読み込み\n",
        "env = make_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "config = Config()\n",
        "\n",
        "# 学習済みモデルの読み込み\n",
        "# rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "# encoder = Encoder().to(device)\n",
        "# decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "# reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "# discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "# actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "# critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "action_dim = env.action_space.n\n",
        "world_model = build_world_model(config, action_dim)\n",
        "agent = build_agent(config, action_dim)\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    world_model,\n",
        "    agent\n",
        ")\n",
        "\n",
        "trained_models.load(\"SavedModels\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a745c7-62dd-4431-99de-4da0f1489a16",
      "metadata": {
        "id": "41a745c7-62dd-4431-99de-4da0f1489a16"
      },
      "outputs": [],
      "source": [
        "# 結果を動画で観てみるための関数\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    plt.figure(figsize=(8, 8), dpi=50)\n",
        "    patch = plt.imshow(frames[0], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        plt.title(\"Step %d\" % (i))\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c8746-7371-4142-bd92-dde301c2f33c",
      "metadata": {
        "id": "206c8746-7371-4142-bd92-dde301c2f33c"
      },
      "outputs": [],
      "source": [
        "env = make_env(seed=1234, img_size=64, max_steps=None)\n",
        "\n",
        "world_model.eval()\n",
        "agent.eval()\n",
        "\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = []\n",
        "actions = []\n",
        "context_obs = deque(maxlen=16)\n",
        "context_action = deque(maxlen=16)\n",
        "\n",
        "while not done:\n",
        "    with torch.no_grad():\n",
        "        if len(context_action) == 0:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            context_latent = world_model.encode_obs(torch.cat(list(context_obs), dim=1))\n",
        "            model_context_action = np.stack(list(context_action), axis=1)\n",
        "            model_context_action = torch.Tensor(context_action).cuda()\n",
        "            model_context_action = model_context_action.view(1, -1) # なんかnp.stackしてるのにshapeおかしい\n",
        "            prior_flattened_sample, last_dist_feat = world_model.calc_last_dist_feat(context_latent, model_context_action)\n",
        "            action = agent.sample_as_env_action(\n",
        "                torch.cat([prior_flattened_sample, last_dist_feat], dim=-1),\n",
        "                greedy=True\n",
        "            )\n",
        "\n",
        "    context_obs.append(rearrange(torch.Tensor(obs).cuda(), \"H W C -> 1 1 C H W\")/255)\n",
        "    context_action.append([action])\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    total_reward += reward\n",
        "    frames.append(obs)\n",
        "    actions.append(action)\n",
        "\n",
        "print('Total Reward:', total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0",
      "metadata": {
        "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c",
      "metadata": {
        "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c"
      },
      "source": [
        "今回，評価を行う際のrepeat actionは1に設定しています．  \n",
        "そのため，repeat actionをそれ以外に設定している場合，repeat actionの分だけ繰り返した行動を提出する形にしています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d",
      "metadata": {
        "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d"
      },
      "outputs": [],
      "source": [
        "# repeat actionに対応した行動に変換する\n",
        "submission_actions = np.zeros(len(actions) * env._skip)\n",
        "for start_idx in range(env._skip):\n",
        "    submission_actions[start_idx::env._skip] = np.array(actions)\n",
        "\n",
        "# np.save(\"drive/MyDrive/submission\", submission_actions)\n",
        "np.save(\"./submission\", submission_actions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}